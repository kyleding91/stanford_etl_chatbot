 You are listening to the DFJ Entrepreneurial Thought Leader's series, brought to you weekly by the Stanford Technology Ventures program. You can find podcasts and videos of these lectures online at eCorner.stanford.edu. Today we have a very special guest and I want to introduce Professor Elizabeth Pate Cornell. Professor Cornell was born in Senegal and she attended schools in Senegal and then moved to France. She ended up receiving her master's degree in operations research and then her PhD in engineering economic systems both at Stanford University. Then she became the chair of the Am Management Science and Engineering Department which is, she had a position she has held for the last ten years. Professor Pate Cornell is a member of the National Academy of Engineering and on the boards of many public companies. She is an expert in the field of risk analysis and today we are going to hear her insights on risk and risk management from an engineering perspective. These are topics that are very relevant to all of those people in the room who are interested in entrepreneurship. Thank you tonight. It's a great pleasure to be here and as you can hear, yes I was not born in Brooklyn. So I'm bringing to you some of my slides so that you can follow perhaps more easily. So I'm going to talk about risks and I'm going to talk about risks from a different perspective. Yes, entrepreneurship. In fact, what I've done in that domain is academic entrepreneurship and the Department of Management Science and Engineering. But what I'm going to try to show you is how we think about risks in the world of engineering using systems analysis, probability, decomposing the problem, using all the information that I'll dispose of and trying to pull all these together in a systematic manner. Okay, so what I'm going to talk about is savings through life, business and service in a world that's longer than us. I mean, but I love saving. And I've sailed particularly in the Mediterranean. I'm not a particularly good seller. And I think that I've learned a few lessons from it. And so I will use that image occasionally. And a world no longer than us, I think is important. You have heard very often, fail often and fail early, but what I like to add to it, but not if you can avoid it. And especially if it is a life or the money of somebody else that's at stake. In other terms, you can do all kinds of things with your own resources. But be careful when we're talking about other people. Now this cartoon shows you three two little characters that are at the bottom of a cliff and there is this very threatening rock on top of it. And then they look at it and it's risk perception. Then they talk about it and it's called here risk assessment. And then they run away from it when it falls. It's called risk management. I don't call that risk management. I call it crisis management. That means the thing is for you, I knew you run away. But in terms of risk management, they could have thought about other things like either making it fall earlier or perhaps tying it to the cliff. Now the problem is that there you are balancing over a sea of crocodiles and be careful. And you have now here a little cartoon where the consultant in risk management is facing is client. And the client says be careful. Or you can tell me is be careful. And what I'm going to show you today is that we can do better than that. So how do we show people how to quote unquote be careful? So what I've learned in real life is don't sell into a store without looking into it. And let me tell you last summer we were selling between Korsekan and Sardinia with friends of mine and they were on a 50 foot boat, not something huge. But the captain said oh, we are living immediately and we're going to put the sales at our mass just half. And I said well, why is that? Well, let's look at the weather forecast and of course sure enough we saw what was coming and it was absolutely right. We left immediately. We had the sale and we made it without any problem. We were the only sale boat in the area. OK, so you have to be confident about what the danger is and what you can take. The first thing that you want to do when you look at risks and you look at what's ahead of you, I am not that interested in what is behind you when looking at past failures except of course to the degree that we learn from them. But probability and logic is a good place to start when you look systematically at the chances of the outcomes and the consequences. And I'm going to start with a few jokes and mistakes and I'm sure you have heard the old joke but the guy was carrying a bomb with him because the chances of two bombs in the same place were small. OK, well that you could not do anymore anywhere. But the problem if you would be caught before you go very far. But the problem here is the problem of dependent and independent events. And what you have to recognize very often is what are these dependencies that you are facing. Now another one was the guy who wanted to eliminate all the pedestrian crossings because more accidents happen there than elsewhere. Sure enough that was a stupid idea. But the way is it that you have more accidents on pedestrian crossings because many more people cross pedestrian crossings. Now given that they cross the road, it's safer to be at the pedestrian crossings nonetheless. So you see here the problem is one of the prior probabilities and the very straight. Now that was a guy who wanted to go as fast as he could on the road because he argued that the risk was smaller since he spent less time going from A to B. Now that was a friend of mine who told me that when I did not stay for very long in his car. And in fact another one of a friend of mine was driving and texting. He was playing with his, and he explained to me that he had done it all his life and he had not had an accident yet. And I said well that's too bad. Of course that's the problem of rare events and that too I did not take and down. So now here is the problem. In the world of uncertainties you will seldom have statistics. And that is where you have to think straight. And when you do a risk assessment you have to look at scenarios. What is it that you are going to face or that you can face? The probabilities and the consequences. And let me say that risk is not an expected value. It's not the probability multiplied by the consequences because very often you have rare events with very high consequences. So what you need is a distribution on these outcomes. Okay, then you can use the result to do several things. One is to check that you can live with the results. And if not, how you are going to allocate the resources that you are going to dedicate to the reinforcement of that system and I'm going to show you that there are many ways of doing that. So first I'm going to talk a little bit about myself. So my own experience was between France, for Africa and then France and then the US. And that was a risk. I mean think about coming here with 200 words of English in the world where there were very few women in engineering, in science and engineering at that time. I remember being slightly nervous about it and deciding to go ahead. I then studied math and physics. Again, that was not an engineering and it was not a very common thing to do. But perhaps the greatest risk, the greatest risk that I took at that point is while the 10 o'clock was ticking somewhere between MIT where I was a certain 12 and here, I had two babies. And think about it, that was a risk that I took and that we took, obviously my husband and I. And I knew that we had a challenge which was to balance the elements of life. It was a personal life and a professional life and make sure that you didn't show change anybody while doing what you were supposed to do. Okay, and talking about real risks of real life, I'm going to start with a very simple example in a very real one to show you all the different sources of data in risk analysis. The risk analysis is not only for airplanes, satellites and nuclear power plants, but consider the following real problem to me. Add a two-year-old little boy who really liked to tumble down the stairs. And we were about to move in a house where on the landing of the stairs there was a sharp post. And immediately without writing anything, I thought about the risk that he could kill himself. So here are the data that I had. The frequency of force, it was roughly once a week. So statistics. But of course, I hope that he was going to stop doing that. I had an engineering model of a baby as a ball, one third of which is the head. How is that? Very subtle engineering. And there was a sharp corner on the landing of the stairs and I had, and I still have an excellent neighbor who happens to be a doctor of emergency rooms. So I said, what do you think? And he said, well, one chance in ten that he might really hit himself. So you see the result if you put all these together from very different sources of data. Gave me a risk of accident of 1 in 30 per week, which was enormous. And I found a risk management solution that was an engineering solution. I put three baskets, soft baskets, one inside of the other in front of the post, tested it to the basket ball, and it's the equivalent in fact of spring's in series that you had it. And I thought to get a gate at the top of the stairs because someone would have left the door. Okay, so the lesson in this is that there are many kinds of different data that you can use, statistics, models, expert opinions. And when it's very important to me, that's the way I think about it. Okay, now let's look at professional risks and academia. I've never created a company. So I'm not going to pretend that. What I did though is to be the first chair of the Department of MSN, Management Science and Engineering. And it was a merger of three very different departments with very different cultures and different personalities, different kinds of emphasis from operations research to social science. And I'm happy to say that 10 years later it's still going strong. Now all risks don't need to be quantified. But I would say that for many risks management decisions, it does help and I'm going to show you, particularly for complex systems and new situations. The most interesting risk problems are with risk that we have never seen. Systems that we have never seen and things that are completely new. The others, that's much easier. Okay, so how do we think about risk analysis and how do I approach it? Well, when I want to explain what I do in life, I say I'm going to see experts and I'm going to say, tell me how it works, whatever it is. So let's talk about satellites for a minute. Let's figure out how it might fail, guidance system, propulsion system, electric system, optical systems. In other terms, any of the critical functions of that engineered systems that you need to have for the whole thing to work might be a source of failure. Then let's find ways to reinforce it. There are many different things that you can do. One of them is to make some of the components stronger. For example, you can have stronger pillars or stronger beams in a building. Another one is to put redundancies behind the system. Another one is to manage it better and to make sure that to give the right incentives to the people or the operators of your system. And we're going to see in a minute why it really matters. So then we're going to find the best way of doing that allocation. And again, as you can see, experts matter enormously. Now as a risk analyst, I'm not making the decision. I'm just providing information to a decision maker who has his or her own preferences. And I want to help that person to the degree that that person wants help. To make a number of decisions, for example, adopt a new technology. As you know, nanotechnologies these days are sometimes feared by some. What's the real risk? This is a system, site of facility. For example, a chemical plant where you want to put it. Managed inspection and maintenance. There, for example, the airlines. I've worked with the airlines for the group of students from the 250V, which is the project class, project course of my, that follows my class. And the question is really, how do you do the maintenance on schedule? How often do you do it for what parents? They are the federal aviation administration, as a lot to say. And how do you do the inspection on demand? And how do you manage that? And then finally, adopt and implement government regulations. The nuclear regulatory commission, for example, makes decisions regarding the safety of nuclear power plants, even though the industry itself does a lot of the policy. Okay. As you can see, I see risk analysis in this context as part of systems engineering. Let me give you examples of things that we have looked at in my group. After all platforms that was after the Piper Alpha accident in the North Sea on a perfect, perfectly clear day, when in fact there was a problem at the interface, essentially, between the riser and the platform and the whole thing exploded in a big, big, baroque fire, but it had started because there was a failure somewhere. And the leak, and the leak was because a young, inexperienced worker had fixed around six o'clock at night a pump with a valve inside, had failed to target, decided to call it a day. No one was behind him. And that thing failed when it was called a penalty, during the night. Medical devices, we have done quite a lot of work with the NPG, for example, one of my, one of my former students, particularly, Horns, Gordiac devices. And the other challenge is to know how to begin to test these things and the test that the FDA now wants on a statistical basis. We take a very, very long time. So the question is, can we begin to compute the probability of failure to stand from? And then do an adaptive testing that allows us to use Bayesian methods to update the information as we do that. Space shuttle, I'm going to show you that in a moment. But also, we have also looked at a system that involves human beings. And I'm going to show you the model of patients risks in anesthesia. And then I found myself, one day, looking at intelligence and counterterrorism. And that's because much to my surprise, in a very non-political way, I became a member of the president's intelligence advisory board. And I looked at the way people were analyzing this intelligence information. And I will show you in a minute a few things that I wanted to put or to inject in that system. So that took me into national security problems and nuclear counterproliferation strategies, which is the thesis of my student, David Casual, whom you know. And the question being, how do you interact with a country that's trying to develop nuclear weapons? So no problem is too big or too small, as you can see. And a wide variety of things, some of which I knew something at free area, I knew something about a free area, some of which I had to rely on experts and learn about that. So what do we do that? We do that because of the answer in this. To make sure, as I said, we can live with the result. And to allocate our resources again, because we are not infinitely rich. So how do we do it by decomposing the system? That's really the idea that systems analysis and trying to find classes of failure scenarios. We are not getting into exquisite details in all the scenarios of how things can fail, otherwise we'd cover the walls. So classes of scenarios so that we can get our arms around that. We include a lot of things that are tricky. Depalances that are very important when failures are dependent in a system. And for one reason, for example, external events. Earthquake. Earthquake will shake this whole building. So all the different components in the building might fare for the same reason. And the earthquake of these external events introduce our common cause of failure. We also include human errors. And believe it or not, people will make the same mistakes for the same reasons in many different areas. And what is most interesting to look at is what kind of information do they have? What kind of incentives do you give them? How do you reward them? And what kind of resource constraints do you put on them? To come back to the story of the offshore platform, there was all incentive at that time in England to push production. And somehow, let's perhaps add the expense of safety. You don't stop the system to fix it. And that was part of the problem. OK, so we try to look at the way people cut corners. Because when you ask people to meet very strange constraints, they will try to satisfy you. And sometimes they do things that you will not like. OK, so do we do it that way generally? Because we don't have enough statistics at the global level. So when we started constructing civilian nuclear power plants in this country, we had some experience with nuclear reactors. When I say, we are not there. But there was some experience with nuclear reactors in the Navy. But there never been enough experience with nuclear power plants at large. And that's where these methods came from in part. Now, the fact is that the systems evolved. So the statistics that you may have accumulated in 15 years may or may not be the information that you need. Think of financial crisis that was exactly the problem. New situations have emerged. And you find yourself with the conjunctions of events that you have never seen before. And that's what some people call the perfect storms. Other way of saying, you know, particularly interest to me are these accidents that have never happened. And let me just talk about a few of them. Concorde was the super sonic transport system, the airplane. And it failed in 2000, but it's not as if we could not see it coming. There had been about 50 explosions of the tires at takeoff because that's an aircraft that has a very small surface of wings. And therefore needed a very long runway. And the technology of the tires at the time was the technology of the 50s. And so regularly these tires were exploding with small lozenges of rubber that were hitting the plane up until the day when it hit the tank and the whole thing exploded again in a bottle of fire. This is one that could have been seen coming and no one had done anything about it in spite of the fact that the tire maker proposed to fix it. Colombia in 2003, that's the space shuttle that also had an accident. And I'm going to show you that yours before we are done study after Challenger of the tires of the space shuttle where the trajectories of the debris that were going to hit the tires coming from the external tank had been calculated and included in this study. And there was also now a success story perhaps, I don't know. Another accident that never happened. The same students who worked with an airline here on the management of a very popular airplane found that there was something a bit strange about the flaps and slats of the leading edge. And the problem was corrected before it caused any accident which after all is what we call success. OK, so now I'm going to get into stories. And I'm going to present to you a few things relatively quickly. So let's start with the tires of the space shuttle. And this is the classic case of an accident that has not happened yet. And so you have to think about it systematically. So we were looking at the first 33 flights. They had not been in a real problem but they were errors in maintenance. And we knew that for a fact. In fact, I spent a month spending a week at Kennedy Space Center with the technicians and the orbiters in my jeans and sneakers, somehow an anonymous sleeper. But what I wanted to figure out is the kinds of errors that they were making whenever in the time pressure. And I found out and I'm going to show you how. So we were trying to compute the contribution of the tires to failure risks. We were trying to look at which ones were really the most risk-quaticum. And we were looking at the effect of management on the risk. This is why the tires look like. Each of them is about eight inches. And the interesting thing is that they are glued on the lattice of filiburns. So each of them has to be glued in a cavity. And in order to gain some time, some of the technicians I found out, one technician at least, that this glue would cure faster if you add water to it. And what he was doing, in fact, was to spill in it. And so the glue was curing faster, right? He was getting time. But of course, that was a dangerous thing to do. So how did we think about it? Well, we looked first at what happens? What's the action mechanism? We have an initial loss of tire or a bunch of them for two reasons. Debris damage, debris hits. Deboning because there is a weak bond. Then at three and three, there is a cavity now. And the flow of gases hits up that cavity. You can then lose additional tires. You expose the aluminum of the orbiter. You can have then hot gases inside the subsystem malfunction, and you lose the roll mission. So the way we looked at this was by decomposing the system into these different parts, getting all the data that we had, that we could. Some of them were statistics. For example, we could get the measurements of temperature on the shatter on the skin. Some others we needed to go to expert opinions. That's what I've put in there. So what did we find? We found that, in fact, the tires were not as bad as the astronauts feared. And we showed them a map on which we had identified the most risk critical tires so that if you had a bit more time to test something before a launch, you could stop there. We made all kinds of recommendations for improvement. Some of them were listened to. Some of them were not. And unfortunately, the epilogue, where the Columbia accident, where again, one of the piece of debris from the external tank, hit the orbiter, and caused the failure. This is the thing of beauty that I presented to the Kennedy Space Center. And what I had put in darker tones were the most risk critical zones. And we had computed the risks in each of the different zones. It's not symmetric because there is a fuel line running on the external tank that weakens the attachments of the insulation. But this was used. It was put as a huge map on the floor. And what they did was to test first and to show me that they had done it, the places that were put in dark. Second case, the patient risks in anesthesia are now again for something entirely different. This is a classic case of dynamics of accidents. So the question was, how can we improve the management of anesthesia system to decrease the patient risk? So what we did is to look at all the accident sequences that could occur in the operating room environment. And so first we looked generally at how accidents unfold, how the competence and the alertness of the anesthesiologists influences the factors of risk and how the management could influence the competence and alertness of these guys. And I'm going to show you what we found. So with the classic model, and there is only one equation in all my talk. I wanted to show you that to compute the probability of an accident over all the scenarios with some over the all the probabilities of the initiating events, these events that stunge an accident sequence. For example, a troop disconnect since the patient needs to receive oxygen in the lungs, multiplied by the probability of an accident, once this incident has occurred. And that's where we had to do a dynamic model. And by the way, this is really one of the problems that you have in general. One thing's go bad, how fast is it going to go bad? And so that you can ask yourself, well, how much time you have to react? Data sources here were really interesting because we had statistics at both ends of the model and expert opinions in the middle, so that we were confident, pretty confident in our results. Now, the way we look in general, as I said, at the effect of human management factors of unrest, in our analysis, we start by the failure modes and the events. We look at the way human performance affects these failure modes, and we look at the way the management policies affect human performance. And as I said, this is general lay incentives and resources. So what kind of management measures did we find to affect an esthetist performance? Exactly what happens everywhere. I think this could happen in the airlines. It could be pilots instead, work schedule, selection, periodic training, experience, supervision of the residents, which turned out to be a critical problem, and of course maintenance of the equipment. Now, for example, we looked at simulator training and we said, well, by how much could we decrease the risks if we were asking people to be on the simulator, let's say, every year. And we asked many experts on the different parts of the problem, what difference would it make? Let me tell you my best expert were the operating room nurses. They had cilitol and they knew exactly how people were messing up. And they helped us identify the accident sequences, what it was, that the simulator could teach these guys. And it's exactly the same problem as for pilots. You prefer that they encounter these problems on the simulator before they encountered them in life. So it was about the risk reduction of 16%, which I think was pretty good. And by the way, the British told me that this, I was a bit optimistic. So perhaps I'm a bit optimistic, but that's what my experts thought. Then, let's go to another problem which is seismic risk. The one that you face every day, if you live in this area. And the other problem is really loans and capacities. What does that mean? It means that, I tell you a given place in the Western US, you have a sudden risk of having an earthquake that's a ground shaking of a particular intensity. For example, the peak ground acceleration. Why is that that building's well? Because the load exceeds the capacity, which is precisely the capacity, the load that which the building fails. So we have two parts in this problem. And you want to decompose it carefully into the two parts. First the seismic loads and for that you go to see the seismologists. And second the structures robustness for which you talk to the seismic engineers. So in here, the key to this problem is to get the right data from the right people. And the use of probability allows you to, again, put your resources across the country in places where they are needed the most without burdening everybody and everything with seismic standards that might be excessive in some parts of the US and insufficient in others. And what do we do it, for example, to support building codes. And I'm not going to get through this model. So how do we use the result, again, to support building codes to tell you what to do with your house if you really want to see that. And after the earthquake that we had, I think it was in 1989, I remember reinforcing ours with the Gensier walls and the solar number of things because it was a good thing to do. But also to assess the robustness of critical facilities. And that means, for example, the bridges in the area, the nuclear power plants, et cetera. And what the system, the distribution system, that some of you in the class are going to look at. Another example. Testing, of course, in the automotive industry. And that was in a German company and a question was to test the whole car we take two days a day. We have only 10 minutes. Now, someone had decided it was going to be 10 minutes. How do we use them? So we looked first at what were the prior probabilities of having a problem in different systems. Then what was the probability that these would be called by the tests and how do we allocate that time. And then I asked the most in this week question, why not 15 minutes or why not 17? So we had a lot of conversations with the engineers. The challenge is what the new electronic system, so I learned a lot more than I wanted about what there is in the electronics of my car. And the problem was to track down the functions and the dependencies because you have a very large number of monitors and computers in these cars. And then psychologically, the problem was to ask people to recognize on 70s, to recognize weaknesses and to qualify them to compare them. Well, at the end, it worked. Let me now give you the example of intelligence analysis. So there I arrived knowing almost nothing about it. And the problem of the intelligence community in all countries is the collection and the analysis of information from different sources. First, you, the issues are the uncertainty about the priors, the situation, the priory. And the insolenties about the information and the dependencies of the sources. And so what I said has a hard joke, as if you hear about the sighting of a very famous terrorist in the bar in Moscow. The priors are very low. And so you will have to check very carefully, not only the source that you have, but even if you have several sources. And we also apply this. And so David Caswell, to the state of a nuclear development program. What are the challenges here? Well, first in that community, the analysts have been trained to think that they are the ones who should quote unquote, make the call. That is, say it is or it is not. And what I've tried to say, and I think with some success, which is do not pick the most likely hypothesis and present it to the boss, whoever the boss is, as if you were sure of it. So, at this point, I think that there has been some difficulties to think about information and dependencies and priors. But the idea that the boss in question, and many buses, would much prefer to hear about the uncertainties than not, has begun to penetrate that community. Interesting. And now again, for something entirely different. One day, I go to call from the entrance, from the entrance consortium. And they asked me, if I wanted to look with some students of mine at the probability of bankruptcy of property and casualty insurance companies, other function of their age, and the function of their size. And I said, why me? Because I'm not a specialist of finance and insurance. And they said, well, that's because we would like to have a system-sengenering approach to the problem as opposed to the classic statistics that are used in the financial world. Fine. So, with one of my doctoral students, Leia Delaris, we looked at what are really the key factors. And of course, insurance companies are becoming investment bankers into long-reconstant. So, first, we got some insights into the industry cycles. And that's because after a big event, the rates of insurance become higher, harder than when the memory declines. Competition is such that the rates begin to decline. And then another one happens. Anyway, the earth industry cycles in the rates of premiums and payments. Second, stochastic process and uncertainty, the performance of the investments, which is of course, where do they put the money, the insurance companies, once you have paid your premium, well, in the market. So, there was there a risk to them. Then we looked at the probability of launch events and claims. And in this case, it was in large part, let's say, hurricanes, let's say, in Florida. So, it may be that this is not stable enough, stable either. And then we also looked at quarterwarns. And we found out that the quarterwarns were growing and were very uncertain and random, particularly in the Southern United States. So we pulled all this together and we used all the information we could, but in particular, we interviewed about 20 retired CEOs, off-entrance companies. And let me say that we learned a lot about that. And the challenges were that they had relied on time series, statistical time series, and in particular, in the financial world, on second moments. So the correlations between the market as a whole and let's say, various securities. And they had difficult days thinking about perfect storms, even though that's exactly what their problem was, or something that's called in recent literature, the black swan, because we have seen very white swans, but not too many black swans. And my question to the author of the book is now that we have seen black swans, what is the probability of a yellow swan? See what I mean? So, what does it take to have something that we have never seen? And that forces you to get down to the fundamental mechanisms of how things happen. OK, so now let's see a few lessons that I've learned. Never do a risk analysis. For someone who doesn't want to know and use the results, we're trying to tell you what data to use. And we're trying to tell you what result to find. So this one has been one of the problems of NASA at the beginning of the space shuttle program. And I think that was too bad. Look out for the small swan, the big storms, the large storms. And the question is that there is a lot of information in small storms about the big ones, because that's where you see how a number of small events can begin to accumulate and their combinations may be very rare and take you where you don't want to be. The role of imagination in risk analysis, that's a wonderful phrase that I found in a 9-11 commission report. The director said and wrote in that book, that was Phil Zellico, that the failure was in the entrepreneur, the failure of imagination. And it was called, of course, an unknown unknown. What it was not that much, 9-11, was not that much an unknown unknown. A similar incident that happened when a French aligner had taken off from Algiers, was going to Paris, was taken over by terrorists, had to stop in Marseille for refueling, and was taken over by the French troops at that point. But it was going straight into, apparently, from what I've heard, into one of the buildings in Paris. So it's not, as if it was totally unknown, unknown. But it is true that it was unknown, and it was going to be extremely rare. OK. So somethings can be imaginable. So you often know a lot more than you do. And yet you can use the seat of the fence, otherwise take the time to think. There is ill crisis. You are not going to begin to do a computation. But otherwise, again, small envelope, big envelope, whatever it takes. I've seen a whole risk analysis for this area on the back of an envelope. Someone wanted to show you, I can do it. It is correct. But I don't care. Just whatever it takes. Warning systems. That's one of the most important points of risk management. Watch for signals and precursors, and remember that they may not be perfect. To test or not to test, there is a point where you have tested enough. And you have to decide on that whether it's on aircraft, whether it's a component, it's an electronic component, or whether it's a medical risk. And I was once trying to explain this question of the value of imperfect information to a senator who couldn't believe the example that I'm going to show you. Suppose that you have a one chance in a thousand of having a deadly disease. And you have a test, that's not that bad. It has only five percent rate of false positive. Now if you start doing the computation, you'll find out that after a positive result, you only have a two percent chance of having the disease. So don't jump yet on the window. Why is it? Imagine that you have these thousand people in front of you. Only one of the disease, one in a thousand. You put all of them to a test. Five percent false positive on average are going to get 50 positive results. And yet, only one has the disease, one in 52 percent. So you see that some of these results are counterintuitive and so the prior matters and the quality of the test matters. Another challenge is to manage the balance between the technical failure risk and the management failure risk, exceeding the budget and schedule. I talked about the challenger accident and the pressures on NASA to launch on that day. But also about managing people who attempted to get corners to meet a deadline. And the story that I told this morning in the course was that of people who were reigned for constructing houses in the central valley. And they were apprehensive deadline. They were supposed to put nails around sheer walls. And time was passing. They put one nail out of two because it was going faster. And during the Northridge earthquake, sure enough, some of these houses collapsed and they are probably in jail for it. But you see, they were given all incentives to meet the deadline and to hide what they were doing. Okay, so many risks are the result of human organizational factors. So as a manager, be very careful of the constraints that you set and of what people are going to do to satisfy you. And you know, I've heard many times that perception is reality. So my response to that is perfect. Let's inject some reality in perception. And that's exactly what you are trying to do by giving some risk analysis results that allow you to put things in perspective. And why is it? It's because you may be scared to death by the headlines that you've seen in newspapers. And that you begin to lose the perspective of the priorities. And if you have a budget and a budget of a country that needs to be spent, it's important that we know what we are doing. And when the risk management is well done, no one hears about it. So success is anonymity. And this is risk management success. You're no longer above the crocodiles, but if you fall, which you hope not to, you have hopefully a soft landing. And that's my story. So Elizabeth, thank you. As most of you know, I'm Steve Blank. I teach MSNE278, the class that surrounds these ETL lectures, Spirit of Entrepreneurship. And we listen to the speakers and go back and actually talk about and analyze their discussions. And the class gets to ask the first couple of questions. So today I'm going to kick it off with a question you almost answered as you were ending. And that is, since you've been on the policy side and the government as well, how did you communicate to professional politicians, the distinction between actual risk and perception of risk? I mean, as a parent, you know parents, nowadays, think that someone is going to steal their child. And but if you actually did the statistical analysis, they're more than likely to choke the death and you're feeding them, then they will be kidnapped. But no one worries about chopping their, does that make sense? Yeah, of course. What's, if you ask me, what scares me the most about my children, I would say, don't drive them. So let's start, start there. How do you communicate the risk to politicians by giving them first, if you have some kind of evaluation, numerical evaluation? To try to give them an idea of how the risks compare to each other. And in fact, I am, I was surprised by how well they understand that. And I've seen a revolution in the last 10 years, perhaps, towards codification of risk and towards wanting to hear not only one possible alternative, but several. I would say the world of success in government has been with building codes. And why? It's because before the use of probabilistic method, there was pseudo quasi-ditimistic and the steel, to look at seismic hazard. Now, it's to complicate and measure that that's a pseudo upper bound of the earthquake damage or earthquake risk at a given spot. The problem is that the probability of getting that kind of maximum credible earthquake is the name of it, was extremely different in different places. So if you use that criterion, let's say in Louisiana and you used it in San Francisco, you would spend much too much money in one place, not enough in the other. And the protections ended up understanding that. I think that Hurricane Katrina was also a wake up call and there's now a lot more probabilistic studies being done about it. Because they realize that you're never going to have something that will be 100% safe. You have to decide at what height you're going to put these liveness. The Dutch, by the way, are being the people who have perhaps in the 50s after they had a really nasty flooding, began to think in those terms and that's the kind of domains in which I've seen progress. Next question is, do you believe, even in your tenure in policy that the American public has less taste for risk now or more risk of worse? And that is your expertise actually more valued or less? Where are we going? I think that there is more information about all kinds of events happening around the world. It's not that there are more risk covers, they are bombarded by bad news. And so what you can do is to help them solve the amount. And yes, there's an enormous amount of demand for the kind of field in which I am. In fact, one of my surprises when the current chairman of the Joint Chief of Staff at the head of the armed services of this country wrote first memo that he held it out to everybody. It was all about risk management. And I thought that's interesting. It probably doesn't hear it or does not understand it in the same way I do. But I think that there is more and more of this idea that there is such thing as risk management. There is no such thing as zero risk and perhaps resource education and setting priorities is important. And I'll just ask you the one last question for me in my class and then we'll open it up to the audience. Stanford has had a long history of advising the government on various levels of presidential science advisory board, intelligence, community, et cetera. Were you aware of this long history and were you connected to part of it or was this a point of enter? And also, what was your biggest surprise about dealing with the government? Well, first let's focus on the call that I received when I was asked if I wanted to be on the president at that time for an intelligence advisory board. I said, what in the world is that? And it turns out that I had friends on both sides of the aisle and I had no idea. I was coming there as an outsider who could look with the call that I had the information that I was getting. And so I did not know much. So my answer to you is I was the innocent, not the man. Let me say that you run quickly. You run quickly by seeing the reasonings and you listen to enough stories that you quickly figure out how you can help. How you would think about it in your world and how you could inject logic and all that. So I was not aware at that time of the long history, although I knew Sidwell, of course, I knew all kinds of people who have been don't shows. All kinds of people in many domains and in engineering here, we have been influential in advising the government. But that's when I discovered how it really worked. And yes, I had surprises. And what was your biggest surprise if I could make it? What was my biggest surprise was the naivete or the unwillingness of people to recognize uncertainties and to want to hear that things are or not. And what you're trying to say is I do not know, but I can't tell you what are roughly the chances, given the information that I have. Now do you prefer me to tell you that they are or are not? No, of course not. But that's my biggest surprise. That they were so the world in black and white, which is not the way I've seen it for many years as you can imagine. Let's open it up to the class in the room. I was just picking up questions for Elizabeth. Yes? I just want to ask a little bit about the side from sort of the engineering approach of this analysis. Are there ways to work with policymakers, aside from what you discussed and other disciplines in order to deal with some of the current challenges we face in security? Oh, yes. The Department of Homeland Security, for example. So security. The question that was asked was, is there ways of using these methods in other parts of the government for example, in terms of security? The Department of Homeland Security has launch programs to do exactly that. And in fact, I have a grant from them just now. And what we're trying to look at, and this is a tiny part, what we're doing is tiny point. But it's walling systems in crisis situations. So what we're trying to figure out is, all of a sudden, you get a piece of information that's a surprise. That's what I call the needle in a haystack. And you follow up, and what else is there out there? So you have to decide quickly what it is that you would like to know and go look for it or make a decision very quickly, given the way things evolve. Homeland Security, of course, is also very interested in ranking the threats, because it's always the same question we cannot put our money, we're not immensely rich. And we are locating all the money of Homeland Security across the different states in the uniform. Why? It's not very reasonable even though there are many senators who would love that. So you have to explain, and you have to help them think through that question of prioritization. Another one that I'm sure you're very sensitive to, although I've not studied it, you do question of airport security. It is clear that some of the procedures in place have a symbolic value, so you feel reflected. Some of them are really effective. And so I know that there are lots of people who are working on these kinds of problems, et cetera. The food chain security, bio terrorism, there are lots of very good studies done by Larry Wein. For example, he had a business call in that area. So yes, there is a lot of work going on at Homeland Security in that domain. Just enough. So you are on the board of public companies. Well, I'm on the board of, yes, one. I'm on the board of several companies of various states. I'm on the board of aerospace cooperation, which is an FFRDC. I'm on the board of InQTEL, which is the venture capital of the CIA and other places. I'm also on the board of a small company, the company that's publicly traded, that does desalination of sea water, et cetera. So these are the kinds of boards in which I sit right now. And the question is, do you bring these possibilities and approaches to bear the needs in that environment? Yes. So, yes, and yes, and yet, even though these sound like companies and organizations that are very sensitive to it, it's not a natural way of thinking. And let me give you the example of the aerospace industry beyond aerospace cooperation. They are in charge of the security of everything that's launched in this country. So you test and test and test and you do all kinds of things. At what point do you decide that you have tested enough? But think about it in different contexts. You go to the hospital and you get a test. And they say, hmm, I see something funny here. And you get a second test. And then you get a third test. And then you keep testing. There is always that question of how much more information do we need? And what is the value of that information? The value of that information is linked to the decision that it might support. If it might make you change your mind and change your practices, that's where you want to have more risk analysis. If you have reached the point where adding more money into more risk studies is not going to make you change anything you don't need to invest anymore. So the challenge is to work between the risk analyst that can tell you what difference it could make. And the decision maker, whose risk attitude is what is going to determine the decision based on that information. Two different functions. Yes. I'm sorry, I can't see you very well. So, I'm going to say that the risk analysis is sitting somewhere, sometimes I'm wondering what it's going to say. So, how is running a risk analyst and considered to be relevant to share a risk? And who is responsible for the studies that is a risk analyst or the decision maker? Let's say, as a risk analyst, I'm generally asked to consider what can go wrong. But when I look at scenarios, I'm very careful. So, the question was how do you introduce the benefits in that balance? As a risk analyst, when I look at the consequences of these different scenarios, I have both the risks and the benefits. And if I have the benefits, I am delighted to put them in here. For example, the risk reduction benefits are building goals. At a cost, you're going to have a benefit, which is the risk reduction. So this is a focus on benefits even though I'm talking about risk. There is another effect that, in fact, I also put in the scenarios. And it is the redistribution effects. That is, because in some cases, some people are going to incur the cost or the risks and other benefits. Think of a large dam. And you live on the path of what would be the wave in case the dam breaks. You're going to take a lot of the risk and there's going to be a lot of benefits to the rest of the population, but there is a redistribution here. And the risk analysis results have to, if they are well done, have to reflect that point also. So the benefits, yes, also enter the consequences. And you have to be careful to make sure that you give a complete picture. Yes. I have a question about just your life and how you've got to where you are. I'm just wondering, your female in a very technical field, and I'm wondering what sorts of challenges you've faced and how you've got to that. That's an interesting one. And on top of that, add my accent. So imagine there you are in Washington talking to people at the Pentagon who really would like to hear your story. It's true. You're a technical field, you're a woman, and you have a strange accent. And with a German accent in a male, it's generally easier to carry around. Anyone in the technical field? Well, let's have a look. So what happened? I really liked math and physics. And so I was undeterred. In my family, let's say, the thought it was not a very feminine thing to do. The medical area would have been more rough. But I really liked math and physics, then computer science. And I really got into that. Then I came to Stanford. And I got more and more involved in things that I found really interesting because they were very practical. And they were really connected to life. And I think that's what I really liked about it. And then once you have done a few studies that people believe in, I believe that. Then it's so snowball. And you have more and more credibility. What's difficult is the beginning. And what you really need at that point is a very good support system. And I'm happy to say that it's exactly what I had. And that's why on top of that, when you add two children, it becomes a bit more complicated. And you don't skip very much. And you make sure that everything gets done. That you say yes to things that you have to say yes to. And you actually do it. And when you can't, you say no. But when you say yes, you actually do it. So it's building credibility and making people trust you. I don't know. Apart from that, the rest we can discuss later. Yes. So entrepreneurs tend to take more risk than non-inventing. And so my question is, is it because they are more risk-averse? Or do they have a better time for risk analysis? Or is it just that they're trying to risk? You know, the fact is that I do not know if entrepreneurs really take that many risks. What I've seen, let me start. What the entrepreneur is the banking system, the financing system. What is clear to me is that in this country, venture capitalists take a lot of risks. And that I've seen. And they generally have a pretty good idea of where they are going with this one. In Europe, it's bankers. And they are not willing to take the same level of risks. It's an entirely different world. Now, but entrepreneurs themselves generally very often they start with an idea, a technical idea. And I think part of the success is to make sure that they have a great team with them. And it's not only the technical point that dominates that small company, but also, of course, the management point to have good managers and good technicians together. And I think that's what the success of their risk taking and their willingness to take risks and the willingness of the venture capitalists to be behind them. There is to have teams. So perhaps it is that the first requirement for good entrepreneurs is to be team builders. And teams of builders with many talents, the management point, the technical part, and selling the thing because after all, without the market, you're not going very far. Yes? I just wanted to know, when you get a really big failure, like for example, the homeless, the MCF in that project, do you scratch up everything that's hard from the beginning or do you work for what you have to do? I'm not entirely sure. Where do I start? Do I start with what I know or do I start from scratch that I don't understand your question very well. So when you get from the very big failure after the planning, and you don't know where to start again, do you scratch up what you had or do you work on top of that? I start from scratch. So when you realize that you're on the wrong track, start from scratch. That would be my recommendation. When you realize that you don't have the right formulation of a problem, go to see other experts, go see what it is that you're missing, and do not get stuck in a rut. OK, if there's one more question, we'll take it. And if not, there are one more question. Going once, twice, three times. Elizabeth, thank you. You have been listening to the Draper Fisher Juvetson entrepreneurial thought leader series, brought to you weekly by the Stanford Technology Ventures Program. You can find additional podcasts and videos of these lectures online at eCorner.stanford.edu.