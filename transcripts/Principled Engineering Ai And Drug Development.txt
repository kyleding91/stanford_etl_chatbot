 Who you are defines how you build. This is the Auditorial Thought Leaders Series brought to you by Stanford E Corner. Welcome to Winter Quarter's final entrepreneurial thought leaders, Sirius Livestream. We've teamed up with the Dean's Office at Stanford School of Engineering to create a special ETL session focused on Stanford's thought leadership in how ethics and principles can shape innovation. Today is the first in what we plan to be a series of events that will engage both school of engineering alumni and Stanford students in vital conversations about the role of ethics in engineering and innovation across disciplines. I'm Jack Fuchs, adjunct lecturer at Stanford and director of principal-dontrapinership at STVP. If you're familiar with STVP, you know that we strive to equip every student with the tools to brave ethical complexity. We believe that if people and organizations have well-articulated principles, they will make better decisions. In my own teaching, we take students on a journey where they develop their own values and principles they will bring with them in their careers. They will then help instill those principles in their organizations helping better navigate difficult decisions. For today's session, we're thrilled to welcome Dr. Russ Altman and Dr. Kim Branson for a conversation about the ethical issues at play at the intersection of artificial intelligence and drug discovery. Russ is the Kenneth Fong professor of bioengineering, genetics, medicine, biomedical data science and computer science at Stanford and is a past chair of Stanford's bioengineering department. His primary research interests are in the application of computing to problems relevant to medicine. He also holds a serious XM radio show and podcast entitled The Future of Everything. We will put a link to this podcast in the chat. Kim is a senior vice president and global head of artificial intelligence and machine learning at GlaxoSmithCline. He leads the GSK.AI team, a global organization of nearly 100 machine learning researchers and engineers who are pioneering the application of AI to drug discovery and development. And since Russ and Kim know each other, in part through their joint development of the GSK AI slash Stanford Ethics Fellowship, which we'll focus on later in the conversation. But first, Russ and Kim, let's begin looking at how issues of ethics and principles first emerged in your careers. Russ, can you begin by sharing some thoughts on how you developed the values and principles that define your work in bioengineering? Yes, and thanks very much, Jack, for having us. We're really looking forward to this. So you listed all those departmental affiliations and the one I want to start out with because it really was my first was the Department of Medicine. So I grew up at the school of medicine and I did my residency there and I was hired on the faculty in the Department of Medicine. And of course, I think no one is surprised that bioethics or bio-medical ethics has always been a thing. Certainly for most of the 20th century, as it became clear that it was important to put some guardrails in place because doctors previously had been doing some very questionable things and really entire societies with some of the Nazi experiments that came out and the Tuskegee incident in which African-Americans were ill treated by the system, by the US government. This has always been present in medicine. So it was a part and parcel of my training. Usually however, focused on kind of two obvious settings. The first setting would be the individual patient and the ethics of making decisions as a physician or a clinician provider for a patient who's sitting in front of you who's depending on you for assistance and needs you to behave ethically. And then of course, the second setting would be clinical trials which need to be always designed to be ethical. And that for example means you can have a placebo arm versus a potential treatment. If there's already a treatment available, that would be unethical to have a half of your patients not even receive standard of care. So just that one example kind of shows you that we had to learn how to think through even the very process of gathering basic medical knowledge and certainly its application to patients. So if I may, I think it would be useful for our conversation to just quickly remind people, many people might know this about one of the most useful frameworks for ethical reasoning. And let me acknowledge that there are many frameworks and Kant had a framework, Kantian deontological reasoning, consequentialism and utilitarianism. But the one that has had the most purchase in medicine is based on something called the Belmont report. And there are four principles that we always use in biomedical ethics. So the first principle is beneficence. You should be doing endeavor to do good for the patient. And that's very important. So you can't be doing something that's null for the patient. It has to be in support. I'm talking about the patient, but that's in any situation you should be trying to do good. So that's beneficence. Non-melathicence means you should not be doing harm. You should not be intentionally. Now bad things can happen in the course of research and in the course of clinical care. But you are trying to not have bad things happen. And that's non-melathicence. And so when you're evaluating a situation, you'll say, am I trying to do good? Am I avoiding doing bad? Then the two other ones, and these are a little bit more Kantian. Those first two are a little bit more utilitarian for those of you who know about that. But it's not important. Justice is number three. Justice is basically, is it fair? Are these rules that can be applied impartially across an entire population and everyone is getting a fair shake? So you look for injustice because that's a sign potentially of an unethical treatment or an unethical clinical trial. And finally, autonomy is whatever is happening being done in such a way that the person being affected, the clinical trial subject or the patient, has autonomy, should use that word, has control of their own fate. Anytime you're taking away control of someone's fate or taking control away from them entirely, that's a red flag for ethics. So whenever we get a situation, we look at beneficence, non-melathicence, justice, and autonomy. That's what the Belmont report taught us. And I think it's a very good framework that, to be honest, I even use in other settings, like my everyday life and certainly in engineering, which maybe we'll talk about later. Thank you, Russ. And turning to Kim, could you share with us some of the values and principles you've brought with you to your work with AI and ML and drug discovery and perhaps how they fit into the values and principles of GSK as an organization? Sure. I mean, Russ, I also, you know, wandered past a med school. But most of my training has been in science. And interestingly in science, you don't often have a lot of the same sort of exposure of ethics that we do in medicine. And maybe that's something we need to revisit. I've worked in building sort of predictive systems in medicine and healthcare for a long time. And one of the things that is a core principle I've always had is that we have data, these sorts of things, because it's a tremendous potential to make things better. But you have to look very carefully at the system you're putting in place in its unintended consequences and the feedback groups you can have. So coming to GSK and establishing the AI group, and GSK is a 300 year old company, right? It's been a long time. It's an extremely trusted brand. We know how to make medicines. You know when you see that little logo on the box, it's going to be safe, efficacious, you know, and it's a really great thing. When you're something now we're building this new AI group, we're going to start using data and building new things. We need to make sure that we build medicines for all people. We want to make sure that algorithms and the algorithm products that we're building also affect all people in the same way. So it's very important for us to have those sort of same types of principles and think about them. And so we are now in a world where a lot of the way we discover the developed medicines is now data driven. And it's all machine learning, it's all feedback loops. So we've realized that anything we build and we put in place gets used, people start to base decisions upon it and use that again, can lead to a feedback loop. So for us, we want to be very careful to understand what that is and to think through the consequence of that. And really, for us, it's a choice of what problems we do work on and what we don't work on, right, is an ethical decision. But a key thing also for this is that we can't decide not to do something because we can't make it like, you know, there's also an ethical not to solve a problem for something if you can't solve it for everybody, right. So how do you wrestle with these types of things? And I think that's something that we know there are biases in literature, you know, who's representing version 8 databases and things like that, you know, and so all those types of questions, they actually come into play in a very realistic fashion as we start to think through what we do. So we have an idea of building software for each one of our assets, right, to help how to use it in a setting. Now the best way we generate data for that is in clinical trials. However, the people that come into clinical trials, there are many biases to who can participate in clinical trial or not. So how do you actually, what efforts do you go to expand and access? And those, there are many questions on that. So all those types of things, when you start to think about this, you suddenly realize that you need medical framework for thinking through these things. Now GSK's company has amazing ethical frameworks for lots of other types of areas, but this is a new thing, right. And so this is where this has really led to actually the idea of, technically, Russ about like, we need a sense of practical engineering ethics for these types of things. These aren't abstract things that we're going to think about great goose scenarios and what ifs. These are real things that are happening now that we actually have to make decisions about. And I think that's delightful to hear Kim, because we teach students that companies should develop a broad set of principles that need to be committed, that need to be communicated, that need to be instilled in an organization, and that will come into conflict as you just described. And that's the wrestling with those principles within the context of a decision that provides the power in decision making. You just described one particular, and I'm going to try to use that as an example. Kim or Russ, actually, then you can also respond. Do you have any recommendations from your experience at GSK of how best to incorporate those principles into the decision making at the organization, or examples where GSK has wrestled with maybe even specifically that value, those principles of, hey, if we can't solve it for everyone, we shouldn't solve it for anyone. Well, that's clearly not right. It's unethical, as you say, to not solve something for some if you can, but where do you draw the line and where is the bias and how is the bias and where do you go from there? So I guess those kinds of questions to you about whether there's tips for people about how to instill this throughout the organization, or examples and situations of how you've had a wrestle with it, and then Russ, you can as well comment on it. Sure. I mean, I think one of the things we have is sort of a checklist. When we're starting to consider to build a product, so as an organization, we only try and do large, impactful things, right? We're not solving someone I want to transfer in this word document to an Excel spreadsheet and building a thing for that. We're building big things in-packed patients, so I hope to discover targets and things like that. And they have large-term, long-term commitments in the company. We discover a target, we do something, we're committing money to it. There's an opportunity cost. There's all these things that happens. So these are big problems we're working on. So the first thing we look at is, well, first of all, let's look at the data you're using, right? And you can't sit there and say, well, this is the data I've got, this data I can find. The first question is, is your data adequately representative, right? Is it got biases if not? And, you know, are there other sources where you could find more data, right? And that's a very interesting question. And then you ask, well, it's sometimes it's not okay to say, I just couldn't find it, it's not out there. So, well, should we gather it? Right? Should we just go and actively gather the data? And that's actually pretty new to most people that come into machine learning, particularly at a school. They're like, what do you mean? Like, well, we'll just buy more. We'll generate more. We can maybe run a study. How do we do this? And we can look at the cost of doing that, right? Now, if it's obviously there are a lot, you know, there are time-bounds and large amounts of money to do this sort of stuff that they think is invisible, so things depending on what you want to do. But you have to make an attempt and we have to have understood that and we pre-specify what biases we know about that. And we made reasonable attempts to address all of them. And I think that there's before you even write a single line of code, right? Just to think through what that is. And then you think the other thing you think about is the what if scenario? I have the model, it's in production, so what? What do people do with it? What's my intended use? Right? And you can write that. Well, that's easy. That's what I want to build a model for. And then what are my unintended uses? If someone else had access to this or what would use this, how else could they use this in different settings? And that's where, you know, I worked with the systems at the past that could predict people's probably of getting disease at time, tea, given their past medical history. Really good for planning care, really good for doing pre-active implementation, things like that, all great stuff. Also really good factor, or methods for insurance companies and pricing healthcare and things like that. Possibly good stuff, maybe less good stuff, depending on the regulatory environment. So you have to also think of the intended unintended uses that you can when you put some into production and how it changed the water random. Those are two anchoring principles, what goes into the machine, and what comes out of us now the side. And we have checklists and things to help people think through that. Right? And it's about thinking through that and to make sure, okay, this is to us what your original intent with your model is, what you how you want to make the world a better place. And then let's work out the best way that we can practically do to accept that aim. And also how do we monitor? That's the third thing. And then we're going to have to do that in practice. So you can't just build these things and life off it goes, now go work another problem. Someone has to have someone has to watch it and log its results and review it. And that's something that also is kind of new that you don't get during grad school and other types of industries as well. I'll leave it for us. I mean, yeah. So I think of, you know, I'm an academic, I'm a professor, and I think of our role as upstream of what, you know, we're sending our folks into Kim's environment and to other environments. So when you think about that, there's a couple of things that we have to do. First, we have to have some formal training in ethics for engineers. And so I'll just briefly tell you the story of the bioengineering undergraduate major is pretty new. And it was actually kicked off when I was chair. And we, there's a process, I'm happy to report that Stanford has a pretty serious process for vetting new majors where you get comments from your colleagues who are outside of the department. And by far, the, and this was in the late two, you know, 2000 to 2010 time frame. So more than 10 years ago, by far, the most common and strongest piece of advice we got from every committee that looked at this major was you must train these students in ethics. And their, their logic was pretty straight line and short. It was you are giving them unbelievable power tools. And this was, this was even before CRISPR kind of came out where now we can literally, but they knew they knew that where bioengineering was going. And they said, you're giving amazing power tools to these folks. They have to have a compass for navigating this. And so as a side of story, I drew the straw. It wasn't short. I'm happy to have drawn the straw for teaching. And so I, for the last 10 plus years with David Magnus, a colleague in the Stanford Center for Biomedical Ethics, we've been teaching a class on bioengineering ethics. It's different from medical ethics because it's more upstream in terms of technology development and whatnot. So and as many people know, our colleagues in computer science have kicked off a class in ethics. The Stanford's current strategic plan has ethics all over it. And I think appropriately. And so we knew this was coming. And so all of our undergraduates, at least, get a pretty good exposure for a whole quarter on these issues. And they write about it. I should say that we have 30 or 40 undergraduates each year. And we're getting 200 people in that class. So there's a lot of people who are not bioengineers who are taking that class. And I'd be happy to talk about it more. But that was the first thing that we need to do for Kim, basically. We need to send Kim employees who have this vocabulary. On that same theme though, we have to practice it in our labs. I think PIs who host graduate students and undergraduates and postdocs. We have to model this ethical behavior and Kim said it earlier. Why are we working on problem X and not problem Y? I mean, a bad answer would be there's more money in X than there is in Y. A good answer would be X is a more pressing societal problem or is part of a very complex. And a little bit of progress in X would make the world a better place in very real ways. So let me be very honest. This has not been a routine way that me and my colleagues choose research projects. I'm not saying that we're mercenaries. But I don't think people have always been intentional about the ethical framework about their choice of research projects. But I'm beginning to see it and I'm excited about it. Because I think that will also model for the students and the trainees that before they go out into their various exciting careers. That this stuff needs to be embedded in your everyday decision making. It's not just something this is a famous saying. You don't just sprinkle ethics on top of a project. Just like a plane starts with the wing, the fuselage, a project has to start with the scientific question and the ethical framework of that question. It doesn't have to be a tone, but it has to be one of the dimensions. And so those are the two ways that we're trying to set people up so that when GSK hires them, they're pretty cluful about all this. Work in progress, not claiming that we're done. Well, it also leads to a question, Kim, kind of a practical question. Russ described, well, a bad answer would be, we work on this because there's a lot of money to be made. A good answer is we do it because it's solving an important societal problem and paraphrasing. Is there a penalty for having good ethics? If you as a company of GSK follows that advice, do they then wind up under performing relative to competition? How does GSK think about that? How do you think about that? I mean, for us, it comes down to really longevity and trust. Right? Like, you know, you need to think with this, as I said before, 300 years of history. You want to continue on, right? Do you want to be the guy who breaks the chain and ruins that 300 year brand? I don't think you do. And there's a reason for that. You have to think very carefully about it. And I think there's some things you don't have to pick up every dollar that's out there. And there are some things that are correct, the correct things to work on. And we kind of fundamentally believe that ethical science, right, is better science. And the reason why it's better science is those types of questions force you to engage more. Well, this is the data I've got. Forced to engage more, forced to understand how to collect the data, why those structures to get the data, it hits you involved in the patient's lives, it helps you understand the systems that have that data in a real practical sense, which means you probably build a better solution, right? A better solution, right? We fundamentally will win out in the market. It's probably more robust as well from machine learning perspective that. So it can be used more. So it is a, it is funnily comes down to trust, right? Making the attempt to build the best thing you can for everybody. And in the process of doing that, you engage more people, you make that you also make it a flywheel effect, right? You're solving problems, maybe understanding why people don't come into trial. Maybe you can fix something to get the data and then there are more people in trials and we, and we actually can come up with better medicines, right? More representations. So there is a flywheel effect to doing that properly. For us, it's not, as us talking about sprinkling ethics about it, it's not about having the ethics police, right? So too often you see people trying to bring ethicists in on top of the team as a regulatory thing or something else coming after the fact or things like that. You can't do it, it has to be done at the same time, right? So you have to think very carefully about the ethical culture you create with the machine learning people. And that's something where we're very keen to have, right? Is the right kind of people. So this is why we have responsible AI people and a VP of ethics and policy around these things because policy and ethics are actually kind of into time, right? That's why we have regulators, right? What are the regulators doing, right? They're actually enforcing a lot of the principles that Russ actually stated earlier, right? That's why we have regulators in society and things like that. So there is a constant dialogue between that. And I think so this is something we've thought very carefully about. And so we have there as they're fully fed to part of the team, but there are also technical people as well. And that's a very key thing. I think that's part of that two cultures thing, something we're trying to sort of change with that this fellowship. And let's ask this question to Russ first and then to Kim. But Kim just mentioned something that that one of the attendees has a question in the chat. For prospective, for prospective founders looking to build in the intersection of ML slash AI and health, do you think governmental regulators and interventions are a reason to worry about progress happening too slowly? And is that a big enough deterrent not to get into the area? And that's the specific question, but let's generalize a bit to the role and the interaction between and among regulators and industry and how that plays out. How you think that plays out. And we'll go to Russ first because Kim, you just answered, but I definitely want to go to you afterwards because it touches on what you just described. Yeah, that is a great question because it's so it seems so obvious that regulation would could only be a thing that slows things down and kind of torpedoes some of your best efforts. And let me say that I've had conversations with CEOs of extremely famous, extremely large tech companies in the Silicon Valley who will say, we will not engage. Now, this is old information. This is this was 10 years ago, but 10 years ago they would uniformly say to me, we will not engage in a project within our company if we see the FDA anywhere near it. And I just want to say that the world has changed and the FDA has changed. So first, two sentences on my credentials on this. I am the co-PI of an FDA-funded center of excellence on regulatory science where we have collaborations between FDA scientists and UCSF and Stanford scientists in areas that are critical for the FDA to understand. And you will not be surprised to learn that digital health and AI is one of the areas that they come to us a lot. There are several of these centers around the company, but around the country, but you will not be surprised to learn that where the one who gets a lot of the interest for AI and ML in health. And we've had several projects with them. The FDA scientists are extremely interested and caring about bringing these technologies to patients. That's their entire professional is to protect the health of the American public while allowing drugs, diagnostics, and that includes and therapeutics, including AI therapeutics. And many of you will know this some have been approved. And so it seems scary, but I have seen over and over again that even for a startup, there is a very well understood way to have a pre-submission meeting with the FDA where you describe the technology that you're developing. You describe everything you kind about it. It's kind of a confidential meeting. So this is not on the public record. And then they give you an assessment of what their questions would likely be. They do that at the meeting, but they also go back think and talk and then send you a letter with these concerns so that you don't have too much of a moving target where they can't guarantee because things happen, but they give a best effort of what you would need to do to demonstrate the safety or efficacy of this, whatever this tool is. And I've talked to startup people and it starts out scary. They usually wait too long for that meeting because it kind of reminds me of a PhD student who doesn't want to do their defense because everything is not finished or even their quals. And I tell them just get in front of your committee and tell them where you are, it will be useful. And it's the same thing for founders. They're so worried about the FDA interaction that they actually delay it too long. You're allowed to have more than one of these meetings. You don't have to have all the answers at the first meeting. And it's extremely unlikely that you say something that torpedoes your entire effort because the FDA is not out to torpedo your effort, but they're out to set expectations. So I hope that wasn't too long, but I'm actually bullish that the FDA is learning how to spell AI understands there's a tsunami of things coming to them and is staffing up or else getting collaborative help like from our center to make sure that they can you know obey all the laws about the response times for various submissions. So so I wouldn't be afraid of the FDA. And in fact, I would engage with them ASAP so that you can see that they're just real scientists who are just trying to protect the public and help you get your product your products out the door. And I'd be interested to hear what what Kim's experience. That's usually for startups. I wonder if GSK has the same experience as a 300-year-old big drug company. And Kim also brought into how GSK thinks about that that regulators and GSK each involved in this emerging set of set of principles. Yeah, I mean, I've had experience from the startup side and also from the company side. And I think the first thing I learned from start stuff is yeah, we waited too long. But turns out the regulators are people too. And a lot of them are really passionate scientists. And I think that's it's really important to understand like okay, we actually have regulation lots of industries. There's regulation running a food truck, you know, there's regulation in if you want to sell your product. This is the people you have to talk to. But actually engage them early on and having a dialogue. And they're quite open to understanding how do things. A lot of it is they're seeking to understand and I'll tell you why why this particular regulation, why they think like this and you can have a dialogue whether that's a case or not. And it's actually one of those things you sort of not talk. You kind of have this thing, the FDA is like the IRS. It's only a bad thing, right? Very different department, right? They're there for a very different reason. And you can engage them. And I think the other thing is that I think people what people fail to understand is that regulatory environment is a constant dialogue between industry, right? And the regulate. So, you know, from the GSK perspective, we should have a conversation with them. I'm like, we're helping develop like what is good machine learning practice, right? For building medicines, right? Should the same thing of how we do updates software for a pacemaker be the same thing of the updating software that I'm trying to do a diagnostic, you know, for a competition pathology algorithm, right? And actually, how even should the regulator validate work, right? So previously in regulatory discovery run a clinical trial, I get all my primary data. We do some statistical analysis. We give the FDA, they check our homework, say we get the same conclusion, there's a debate. I'm glossing over this in horrible terms and probably, but that's kind of the process, right? And off they go for machine learning algorithm. Why doesn't the regulator have their own independent set of data that I don't have? Right? It's an API. Why don't they decide they're going to call it anytime? Why don't they can continuously monitor something in production if they wanted to? Right? We do have monitoring and production, right? Now in the pharmaceutical industry is adverse drug reports, right? There is a parallel for these types of things, right? So what you have to do is have a conversation with the regulator, you need to talk to them about this stuff saying, hey, we think this is how you could do it, right? And also explain to them the types of people they need to be hiring or training, right? You know, they need software engineers, right? They've got really great statisticians at the FDA and people like that because they've needed to have them to understand your trial design. So it's an evolving concept. And I would say they're more you engage with them and told them what you want to do and how to do something. You know, you get to know people there, right? And you can actually as a small 20 person company or a giant pharmaceutical company have these conversations with them as well, right? They, and I think that's a really important thing is to engage in their dialogue and the regulatory environments are there for reason. Now I may not evolve in law as fast as you want, right? But this is where I think that typically there is always a path to work with them to do something, right? Because as Rasa, they are highly motivated, right? To try and change healthcare, right? Because the other thing is if they would like to say no to everything, right? And I've probably been able to say that there was a famous regulator who said no to everything that allowed a single drug in their entire career. But that's that also that doesn't work, right? And that leads to a backlash against that sort of thing, right? And then once that happens, we lose, we end up having building things out as safe, right? So there is that interplay. So I think it's very important to engage involved and realize it's an evolving thing and you can push it. It's the laws aren't static, right? You can campaign for change and give really strong reasoning for the thing that and it's an educational component as well. And you'll learn from them, right? Thank you. Thank you. Both of you. And Kim, another question from the audience. Kim said, we only do big things. Address the ROI of not looking at rare diseases when a corporation doesn't see enough market share or market size is probably what they mean for a big return or even a return. Let me make it a little bit harder. This is one of those wrestling with the financial return versus solving an important societal problem. Sometimes the answer to that is to charge an insane amount for the drug in order to make the market size big enough. So how do you wrestle with that at GSK? Yeah, well, I say that we don't do big things. I was talking about like the objectives of the AI department. We don't work on a few things that we think are very impactful, right? Have thousands of impact within GSK. But if you come down to the landscape, so in many ways the cost to run a trial and things like that can only get so cheap, right? No matter what you do, whether you're doing it for one patient or without, you know, things like that, there aren't. There's just a sitting, right? So those are some, just the economic realities of running trials at multiple centers and all sort of overhead that goes with things. So in order to offset that, there are various types of incentives, right? And tax structures, orphan drug acts, things like that, rare disease acts. And what you actually have is a set of companies that have built, built therapeutics for those rare diseases, a single men, divin, genectors, orders and things like that. And so, and they are typically smaller companies, right? So they have lower overhead. Maybe they've done a little longer research phase, and then they sort of maybe shut that down and go to production to conserve the capital and do that sort of thing. That's the way we've typically seen those sorts of things being addressed. Now, there's a lot of those companies and in many ways, a lot of those easy to easy to build a medicine rare diseases are getting done. And you're seeing some interesting things, right? There are, you know, there are, and it's not that they're always done by small companies. Sometimes they're done by very, very big companies as well, right? But the way they get around that is through the various sort of incentive structures of patent life cycle and pricing and things like that, right? And expedited review, the standards are sometimes that we apply to that for a rare disease, but say, if you're giving a medicine to a small population for which there is nothing else, right? And maybe it's 50 or 100 patients, different ethical risk, risk reward rate risk, camera issues are used compared to, I'm going to give this statin to most people aged over 50 and they're going to take up a 20 years, right? Very, very different risk role, probably, with you very careful about that. That all comes to regular sort of stuff. So it's not something that like, you know, oh, we only do things if it can make a billion dollars out of it, right? And I think the other thing to remember is, to be honest, the age of the billion dollar blockbuster drugs is going away, right? There are very few of those left to find anymore where it's the one medicine that works amazingly well and all comes, right? There's always competition in these different types of things as well. So what it actually is now is the way of finding cheaply and effectively the patients for which your drug is 15 times extra effective than in another population. So differentiation now is the name of the game, right? And that's the trend, right? Rare diseases are really nice because they're a very well differentiated population because they are rare and you know who has the perhaps, right? It's it's the discovery thing. So there's a lot of things that go into play for that position. Great. And shifting gears a little bit to Russ because I want to make sure and we'll also ask him this question too. I'm thrilled with this with the partnership that you two have been working on. Staff Stanford faculties on the forefront of ethical thought leadership applied to technology and often in collaboration with industry. And this gsk.ai Stanford ethics fellowship is a case study and I mentioned it briefly at the outset. It's a new postdoctoral fellowship that allows researchers to study ethical considerations at the intersection of AI and machine learning and drug discovery. So first Russ could you briefly touch on what led to this kind of postdoctoral fellowship and how it was designed? Yes. So I don't know if this came up in the introductions but but Kim as a youth spent time at Stanford. And so I know Kim and we we we we meet every now and then and we talk about stuff. And so I'm going to have to give a lot of credit to Kim because I had my draw drop one day when a representative of a major pharmaceutical company who I also knew when he was a postdoc came up to me and really said I don't want to say drop it in my lap but he said we have a pretty clear idea Russ. He made the arguments that you heard earlier about why this was absolutely critical to gsk. He knew that we we had some presence in bioethics and biomedical ethics at the medical school and at the engineering school which is very important because this is a you know this is a bridging kind of issue because of the AI as well as the medicine being involved. And so really Kim's challenge to the Stanford faculty was how could this work and how fast can we get it up and running because you know I work for a big company Russ and we don't have the same timescales that you sometimes have I need it yesterday. And so we we straight away started talking about how it could run fortunately. The Stanford Center of Biomedical Ethics has a long history of training fellows. It especially fellows with technical backgrounds. So that fell in to place very nicely. We kind of wrote down some of the scenarios that somebody might research. We worked out all the intellectual property to make sure that you know very importantly this has to not look like we're shills for jsk and there has to be kind of academic independence and yet gsk is paying for a lot of the training and so we have to make sure that the outputs are of at least of relevance and of interest. So working through those conversations was fun and challenging but not killer. And then we announced this program and we're actually recruiting for it actively. They will have full academic freedom to focus in the area. I think that let me just say that the big new idea here is ethics in clinical trials for drug development as well established. But what Kim said to us that really got us excited is we want to focus on the very early stage discovery when you're doing genome-wide association with people's genomes when you're looking at cellular data about how diseases. So this is very early when it's not about patients it's more about those issues I was discussing before the choice of problem the choice of whose cell line are we going to discover make discoveries on which population genome sequences. So I'll let him chime in on this but that was the genesis of the project and now we're going to have people come usually post PhD spend a couple of years with faculty mentorship and kind of a collaboration with relevant gsk scientist and then try to make traction on some of these tough problems. Great and Kim how did you navigate the waters of gsk to make this happen. I think Russ's description is fairly accurate. I think one of the things we realized and it came back to the way we're discovering drugs now is it's a lot more of a state of driven right so it is genome-wide association studies it is and that gives you a hint of which genes you do you might do the function genomics now right so this is Christopher another technologies we have induced periproton stem cell lines right which donors are we taking those from right you know is it just one line or a panel of donors right and all those things when you put together right I think if you know I've done the G was this population if a minority group isn't represented there right and maybe their gene there's a different risk a little for that we don't discover it and you realize something when you chain all these things together they have a reinforcing effect and those are things that come out the other end is the tie that we're prosecuting making medicine do it running to a trial and you realize all these things individually like oh I'm not using stuff with clinical with clinical patients so it doesn't it doesn't apply but actually when you step up a layer look at this as a systems level type approach you realize like oh yes it really matters what goes into the machine where and how we use a data and how we make the decisions that each gate as things flow through and it was also driven by you know you when you have a conversation to an angle like we're going to have a code of ethics so I was like oh yeah sure I read a good ticker Kim moment no no it means something like like it's not just you know you have to ticker thing compliance right because we have we're used to doing with a regulated patient data and things like that the problem that we observed is that a lot of the people in ethics literature are not technical not computational so typically the way they've addressed problems or talked about things are either so remote there's like I don't know what to do with that like an agi type problem or just like you know just not relevant right not practical they're like okay that I see that what what should I do help me so the idea was like well actually what we need to do is we really need to do research in ethics right so we have things right now and like there are whole things like for instance it is a concept of what I call a data code chain so a code chain is something I have so I can move which we will now about like you know vaccines across the country and I can do things keep refrigerated and safe so I built a really great algorithm there's computation pathology and predictive various outcomes and things like that if you're in a country that doesn't have the data infrastructure to do that you can't benefit from it right so what does that mean do we be investing in the data code chain for those companies should be a device and some stuff or should we just say well you don't have it but you know it works great if you're living in a if you're living in North America right all the UK or something like that and so you realize that there isn't there are a lot of issues around this sort of thing but there's also research about how these things are used and like because we don't know these are all new we need to actually start doing the research now and rather than than like just doing it and then saying oh that was bad now we write some policies about it like actually let's get ahead of the game and try to do that so let's study it as like learn from the previous approaches happened in medicine right as Russ said all those sort of things that happened previously we can start doing research about that what are the implications so that's the idea of like let's create a fellowship that also has people that are that are technical trained right that will learn these types of things so that really and to be honest I would say a fraction of the AI community does simply ignore a lot of the ethics type stuff right like it's not as it's not as bad as it used to be it's definitely changing people are realizing that but for a long time they're like they just kind of ignored it right there's like no I've got steady out I've done this sort of thing right I'm focusing on my facial recognition task it's really awesome my my models 10% better than than this other guys look at my new architecture like yeah it's a cool problem you're focusing on but it is in a wider context have you thought about that and now people are thinking like oh yeah that's probably a thing right so we want to do the same thing now right in our particular niche right and it's I we think it's fundamentally going to be about research driven to lead practical solutions right very good and and I'm trying to do as many of the audience questions as possible I still have a few that will probably get to but on on the other side but from the audience there's always a lot of hype in the media about the use of AI and drug discovery and development what's the true state of the situation what are the areas where AI and ML does and does not work is the way they put forward or let's say where is it more promising where is it potentially over hyped I'll start out this is what Kim does for a living so that I definitely don't want to follow him on on this question but but but I do I think about this and I do I do some kind of consulting and I've seen some so here's what I see first of all for the very very large genomic data sets AI is pretty much mandatory to pull out the signals of which genetic variants especially in combination are correlating with the phenotypes that is to say the diseases of interest so that some of the big data is almost big enough to impress the folks who really deal with really big data you know the the Google Facebook Twitter people are in a stratosphere we are below that but it's pretty respectable big data at the genome so that's the first thing second of all everybody knows that AI is good at detecting patterns and this can be very useful when looking like looking at electronic medical records for finding a bunch of patients whose disease looks similar a lot of diseases are really waste waste bins of like all different kinds of people with slightly different diseases and when you do a clinical trial of course your drug is only going to work on 20 percent of the people because 20 percent of them actually have a version of the disease where that drug is relevant and any 80 percent might not even have the disease or they just have a totally different form the AI systems are very good at finding patients who are kind of looking very similar along all available dimensions and that's also very valuable. Now jumping very molecular you've we've all heard about alpha-fold 2 alpha-fold 2 is the program out of deep mind that was able to predict the three-dimensional structure of proteins I'll just remind people who haven't taken biology since high school that those three-dimensional proteins are typically what we call the targets of a drug. A drug is often a small molecule that binds one of these proteins and modulates its function to help the patient have a you know have their disease go away or get better. So going from just some protein three-dimensional structures to all of the structures at at least almost all at a reasonable level of accuracy opens up our ability to think about in a very rational way new drugs to interact with proteins that we weren't able to to think about previously because we didn't have the structure and and so that whole three-dimensional structure and molecular understanding of drug action is about to be revolutionized. I mean it's happening right now. I can't tell you how quickly engineering students read those papers walked into my office and said I want to work on the spin-out effects of that discovery. It took about a month for them to figure out and so so I'm sure Kim has other examples but but right now those are the areas where I'm seeing a lot of excitement and a lot of positive results even impacting successful drug launches. Yeah I think drug discovery development is a big big set of fields right they intersect there so there are as Russ said there's a lot of things that are happening in you know the early discovery phrase so certainly you know yes G-Wars. Now G-Wars itself is a pretty old technique right multiple sequential independent hypothesis. You better define G-Wars. Oh G-Wars association studies right so we think like what that's still how we do things except we're now building that we're now putting models that look on Rodee and A-sequence and predict open and closed chromatin and these are stacked in code of models right for doing this sort of thing that explain how things can change across different cell types. We frequently do these very large function genomics frames where we I can do every single gene up and down by the pairwise types are for these various poor crisper things so the methods like perturb seep they generate very large data points like I think last year we probably generated I don't think 25 within data points for as a feedback loop for a single ML model we're building so there's something we actually generate data just to feed into our algorithm so the algorithm becomes a discovery tool so that probably gives you an indication of how important it is there's been amazing advantages obviously in computer vision well that now applies to what we're doing with cells cell of phenotypes I can track things over time adding time to everything is key for biology right because these are dynamical systems that change I can now do single cell RNA-6 I can take single cell and I can do genome sequencing RNA sequencing of that right and I can do that over time I can look at cell homophologies I can look at protein expressions as well functional characterization so now we have all these multimodal data right it's really difficult to integrate so one of the great methods and particularly around neural networks for some stuff is transfer learning and and multimodal integration so that's the all these things come into about not so much like I'm talking mostly about what is the target what is the thing you should make the medicine about right so even in doing that we have an AI system that we use for even he's at he's where I want to drive my cellular model to my clinical translation model to right what is the best target for doing that now I could just do one gene of the higher and 20,000 things and I'm using CRISPR to modulate it rather than making a small molecule tool that's something super new or I can actually be a bit more smart and I can turn everything into sequential learning problem takes in data generates in data have a multimodal feedback where I want to try and make the thing look like this thing express the same genes and it maybe make this protein a higher level right that's going to be a really good drug target that helps you discover something and then from that then it's like it's the design right so it could be a small molecule like Russ is talking but it could equally well could be in anybody right and now we use you know Gaussian processes and also the Bayesian optimization to optimize the antibody sequence right for various properties not only just binding but aggregation, lyophilization stability because it has to last for a long time right so it starts to feed into the manufacturer aspects it goes into the recruitment aspects as well and so we see you know you have this lab in the loop of all your models but we also have the clinic in the loop right so the Russ is early a point a lot of diseases are you know made up by Victorian men and top hats right they're all symptomology right but when you but now we can measure things on such a fine scale so we can get a population of people who have disease and observe them and start to characterize things what's going on like let's be a Parkinson's right some people progress really slowly some people progress really fast right is are they all just the same Parkinson's or is there a different pathological process over there we want to sort of dissect that out because it tells us something about things so we see a lot of impact in across across all the sorts of domains even in manufacturing and scale in selecting patients operating trials right we have a lot more senses and wearables now that we can use in trials as well so even our schedule clinical as assessments is changing so it is one of these things that it's not just one revolutionary thing that you put in the sequence of the person and here's the here's the molecule you make right but it's across oh these little changes across the entire big pipeline will speed up the whole thing right and I think for me the biggest impact for something we discovered was if you have targets have genetic validation and functional validation they're about twice as likely to become really successful medicine even if you left the rest of the whole machine unchanged right so it shows impact the pulling the right working on the right things in that pipeline so there is there'll be lots of change happening you know across the industry right thank you yeah there are the criteria that are that have emerged in the broader AIML world of trust security fairness as being principle you know principles that people evaluate technology against are there are there adaptations of that framework or or other vectors that are particularly suited to AI and ML as applied to drug to drug discovery those are good ones and you know in general you have to be thinking about those there are a lot of detailed issues that you I'll keep this short because I can think we have a lot of questions there are a lot of detailed issues that are a little bit different in biology maybe there is responsibility because there's liability in medical devices and drugs there the the the the chain of responsibility for decisions has to be somewhat transparent or or or a bunch of lawyers will make it transparent so that that's on the mind for the for the AI system explainability which has come up in all areas of AI it's it's controversial because there's a difference between the explainability you need when you're convincing people that it works and that there that might be different from the explainability you need once it becomes a routine part of practice one thing that I want to stress and this is really relevant for all AI systems is we've talked already a lot in the last 53 minutes about the importance of data to inform our AI systems once we start deploying AI systems then the AI systems will be affecting the data that we collect and it's going to be a very complicated situation to to unravel why we're seeing a new set of biases so we have all the old biases and then we're going to have a new set of biases which are the biases that came from using an AI system in the first place that didn't used to be there and I'm very a concerned and be interested in research that starts to untangle how do you update an AI system when part of the data that you're training it with is the previous version of that same system very complicated there are some lessons from that in the sort of the people who build financial trading avenues because once they put it into it it changes the market right and so there are some things to learn from that but I think to the point about sort of interoperability a lot of that is sort of robustness and reliability concerns that people are trying to address right because if you ask people you know we all interface with algorithms every day lives and things like that or technologies right how does an LED work right how does this monitor can follow me work and I just explain the physics of that not all but it works I'm pretty I'm sure and it has a minimal impact to me so I don't really need to know so a lot of the time what we where we want that is we want to make sure it's robust and reliable like why is it make it make a decision with inputs and let's tell people get carried by interoperability a lot more when there's a potential for harm right and it all comes back to ensuring that is a robust system and we know some AI systems right particularly some of the visual ones I flick a few pixels and things like that and I turn a banana into an owl or something like that right or what happens if I've got my computation with the algorithm which is classifying someone's tumour stroma boundaries and other types of properties it's slightly out of focus right or I've got folded tissue how do I show this robust rival and safe right and you know certainly the pathologist look at things he may have off days maybe he doesn't look at all the slide things like that there's noise with that but we can give it to two people right and we know there's noise in the system so there's many different ways to think about that and I think these are some of the areas that need a lot of research right and and are going to be critical to get this because we are sort of taking something into it and we're going to have to run things in parallel with the old system right it's not going to be something we're just going to cut out over overnight right because we'll need that observation but then we also know that like sepsis prediction algorithms that like the ones that I guess are probably made by various EHR manufacturers right if you look at the look at what's happened now you look at their performance drift their performance decades over time because medicine is in static we get better at understanding recognizing sepsis and things like that so that so the signal strength and maybe that's only the harder patients right to detect right so there's a lot of things that go into this thing that as much as once you put them into practice and I think that's something that we will have to consider as well particularly as we gather other data about people right all right let's try to get one or two more questions in the last three minutes here uh someone asks about penalties for working on ethical problems uh about whether there's a biology tax in that if you work on AI and ML for biotech you got you're going to get paid less um does that disparity bother you and how do you how do you think about addressing that so let me address that because I have some specific experience in this it can definitely be the case uh there is a class of machine learning employees who I would call quite mercenary uh and you know I've had a startup and we had AI people and I talk to the chief uh the chief the CEO and he was very clear he said Russ this is a tax we can't pay as much as Facebook and you know Google but it's not that hard to address because our mission is very compelling so if you're in front of a one of these mercenary people it is not hard to figure out that they are just going to the highest bidder and you do not hire them yet you find somebody who understands that the mission of curing cancer or making new drugs is a worthwhile mission and and they can and also of course they'll still be able to live and take care of their family um but some of that big bonus that they might get they're getting it because they're working on the help system for the help system and okay god bless you but maybe you want to work on drugs that are going to cure Alzheimer's so you have to do that at recruitment time is the advice that I've seen work because you can compete simply on monetary reimbursement but we do have a couple of knobs we can turn because it's a pretty uplifting mission okay and uh just in the interest of time 30 seconds each just on where do you see the principles that we've all been talking about today evolving what you know we talk about the importance of principles evolving uh here uh just a quick critical sentence each go ahead Cam I think we're going to look at the time where we sort of rolled out for these recommender systems and stuff in society to be the equivalent of sort of Victorian chimney sweeps and like you know a cast of Jenny Kasselton cat compounds but like I can't believe they used to do that so I think that it's going to become just a part of life we start to think about these sort of things it's a new technology that's emerging right so I think we'll come back into everything Russ I think it's going to be about understanding governance uh and embedding responsibility throughout the organization the the ethical theories need to be expanded to think about distributed responsibility they're not very good at that right now um and they have to be expanded to understand how governance decisions have a direct impact on uh at the ethical decision making the entrepreneurial thought leader series is a Stanford e-corner original production the stories and lessons on Stanford e-corner are designed to help you find the courage and clarity to see and seize opportunities Stanford e-corner is led by the Stanford Technology Ventures Program and Stanford's Department of Management Science and Engineering to learn more please visit us at e-corner.stanford.edu