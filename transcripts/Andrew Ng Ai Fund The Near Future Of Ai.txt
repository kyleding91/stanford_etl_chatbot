 This is the Entrepreneur of Thought Leaders series brought to you by Stanford Econ. Welcome everybody to the Entrepreneurial Thought Leaders seminar, a Stanford seminar for aspiring entrepreneurs. ETL is presented by STVP, the engineering center, the engineering entrepreneurship center here at Stanford, and basis the Business Association of Stanford Entrepreneurial Students. I'm Ravi Belani, a lecturer in the Management Science and Engineering Department at Stanford, and the Director of Alchemist and Accelerator for Enterprise Startups. Today we are thrilled to welcome Andrew Aing to ETL. How many people know Andrew? Okay, so Andrew really doesn't need an introduction, but we will give one anyways for those who don't. Andrew is truly a child of the world. He was born in the UK to parents who emigrated to the UK from Hong Kong, but was raised in Hong Kong and Singapore, went to Carnegie Mellon and very early on signaled that he was no ordinary student. He got three bachelor's degrees at Carnegie Mellon in computer science, statistics and economics and graduated at the top of his class, then went on to MIT where he got a master's in an auditorium during a computer science, then came over to the left coast and got a PhD in Berkeley in computer science with a focus on artificial intelligence and reinforcement learning. Andrew is generally viewed as one of the pre-eminent thought leaders on AI today. He was the founder and head of Google Brain and the former chief scientist at Baidu where he built the company's AI group into several thousands of people, several thousand people, but he's as passionate about AI as he is also about the development of you, of students around the world. And I know there's a lot of love for Andrew. He's a former associate's professor and director of the Stanford AI lab and currently an adjunct professor in computer science at Stanford. How many people have taken one of Andrew's classes or want to take one of Andrew's classes? And he's a beloved professor here at Stanford, but he's also viewed as a beloved teacher to millions outside of Stanford. He's the co-founder and chairman of Coursera, the world's largest MOOC platform and through his online education work and his online AI education work, he's reached over seven million people. He was listed as one of the world's hundred most influential people by Time Magazine in 2013. And today, Andrew's the managing director and partner at the AI Fund, which is a startup studio building new AI companies from the ground up and is also the founder of DeepLearning.AI. He focuses his time primarily on his entrepreneurial ventures, looking for the best ways to accelerate responsible AI practices in the larger global economy. There is fantastic content already online that Andrew has given, including a longer version of today's talk that you can find on YouTube. And so instead of re-duplicating that, Andrew's going to give a teaser talk, a 10-minute discussion, followed by, we'll do a quick fireside chat, and then we're going to open it up for really interactive Q&A with you. So start thinking about your questions now because the time is going to fly by, but without further ado, please welcome Andrew. Thanks a lot, Robbie. Thanks. Good to see everyone here. Can everyone in the back here be okay? Cool. Awesome. You know, I've told CS239, my machine learning clause in this room many years, but all these years I've told that this room I've never seen my face that big before. What I'd like to do today is chat of view about opportunities in AI. So one of the difficult things to understand about AI is a general purpose technology similar to electricity. Meaning it's not useful just for one thing. It's useful for a lot of different applications. If I were to ask you what is electricity good for? It's almost hard to answer that because it's useful for so many different things and AI is like that too. So one of the major trends that we've seen in the last year, few years, is that prompting is revolutionizing AI application development. And I want to just dive a little bit deeper into this because I know there's an engineering clause. I know many of you may be from an engineering background. I'm going to just go a little bit deeper than this into this then then I might otherwise. But if you were to say want to build the AI system for many years, the typical approach is to use supervisor earnings. So let's say I want to build a system to rate restaurant reviews as positive and negative sentiment. Then you would collect data. Maybe that takes me a month. I would train AI model. Maybe that takes me a few months. Find a cloud service to deploy my AI model. Maybe that takes a few months. And so for the past, most of the past decade, a realistic timeline to build and deploy a value AI system was maybe six or 12 months. But with prompting, the timeline is now very different. You can specify prompt in minutes or hours and then deploy a system to production in just hours or days. And I know that probably many or maybe most, maybe all of you will have played with large language models as a consumer to like chat GPT and bar them being chat. I think that in terms of startup opportunities, I'm excited about the use of large language models, not as a consumer too, which is fantastic and exciting. I think you should use chat GPT regularly. But instead, the application of large language models as a developer to because this is allowing a lot more air applications to be built and dramatically lowering the barrier to building many applications. So I know that in this talk, you don't normally have speakers right code, but this is an engineering class. So let me actually show you exactly what I mean by that. It turns out that if I want to build an AI system today, this is all the code I need. And this means that if you take CS106 or something, you're going to code in a CS class, with just a little bit of code, import AI open I2s, load my key, I don't know, D what ST, P lectures, I'd rate, you know, so many friends. Never written that before. And so hopefully this, okay, thank goodness got that right. And so this is positive sentiment. And just in seconds, you know, that's all the code it takes now to build an AI system in code to look at the piece of text and process it, to look at piece of email and route it or to build a start to build the beginnings of a chatbot. So over the last half year, one of my team's deeplimin.ai has been working with many of the AI2 builders to create short horses on how to use tools like I just showed you. Because there are many AI applications that used to take me six months to build that I think any of you will now be able to build in one or two days. And this opens up the set of things that you could do and to set the prototypes you can build. And in fact, from a start to perspective, when it took us six months to build something, you know, what we do is have a private manager study it, do the user studies, make sure it's the right thing to build, then go build it, and after all that investment is like, boy, let's hope it works. But what I'm seeing with these very fast development times is if it takes you a couple days to build something, I'm seeing a lot more startups as well as big companies say, you know what, I have 10 ideas for features. I'm going to build all 10 things and then just ship them all. And then we'll see how users use them or don't use them and just keep what sticks. And this is a very different prototyping, much lean in methodology than I've seen startups use before prompting. We've won one important caveat, which is that response for AI is important. So don't do this, don't ship things that could pull some, but we have a lot of applications like inspecting bits of metal and factory, you know, where there is really no harm, no risk of bias, where I think there's very fast shipping methodology that's just innovate very quickly in AI. So where are the opportunities? So the size of these circles shows what I think is the value of different AI technologies today. Supervised learning started to work really well about the decade ago at labeling things, such as label does that, is it something you likely to click on or not, or label this x-ray with, you know, what's the medical diagnosis. And supervised learning for a single company like Google is worth more than $100 billion a year and there are millions of developers working on it. And it might even grow in the next three years to double say. So massive momentum, lots of applications to be figured out. And then genitive AI is a new entrance where frankly the revenue, the value of the revenue from genitive AI today is much smaller, but giving the amount of interest and excitement and commitment interest I think it will much more than double in the next years. And three years isn't artificially short time horizon. I think it will look out six years if it continues to compound at this rate. Maybe the value from genitive AI will, you know, even start to approach that the supervised learning. But all that room for growth, the light shaded region for supervised learning or genitive AI, which are probably the two most important tools today, are where there are a lot of opportunities for any of us to identify and build to concrete use cases. And one of the hope to take away from this talk is AI technologies on general purpose technologies, meaning that they're useful for many different tasks. When supervised learning started to work well about a decade ago, it actually took us a long time. It took us a knowingly long over the last decade and it will be take us a knowingly long over the next decade to figure out use cases for genitive AI. But do you want to use this to make ships more fusion or for medical diagnosis or for education, private recommendations or something else? It took us, we're still figuring out concrete use cases for supervised learning. And even though we're not yet done doing that, we have another fantastic new to genitive AI that even further expands as to what things will now do with AI. And, you know, one important caveat, which is there will be fads along the way. How many of you remember lensa? Where is your hand if you do? Wow, almost no one. That's fascinating. So lenses, revenues, took off like that through last December. It was this app that could let you upload a few pictures of yourself and draw a cool picture of you as an astronaut or a scientist or something. And it was a really good, really hot product until last December after which is revenue did that. And I think that's because lensa was one of what will probably turn out to be multiple thin software layers built on top of someone else's very powerful API that was a good idea of people liked it, but it wasn't a long term, defensiveall business. And when I think about genitive AI as a developer platform, I'm reminded of when you know Steve Jobs gave us a stone, and shortly after someone wrote an app that I paid $1.199 for to do this, to turn the film into flashlight. And this was also a good idea. It was a great product, but it just was not a defense ball business either because it was very thin software layer built on top of someone else's very powerful development platform. But in the same way, after we got the iPhone, after we got the smartphone, someone else figured out how to build Uber, Airbnb and Tinder, much longer term defense ball very valuable businesses that are still standing the test of time. And I think what those opportunities as well to go long term valuable franchises businesses on top of genitive AI. So where are the opportunities? So I felt I felt years ago, but even more strongly now, that because of emerging AI technology, there are a lot of projects that are now possible that we're not possible, you know, one or a handful of years ago. And I wound up starting AI fun, which is a venture studio that sequentially works with entrepreneurs to start companies. We actually average about one startup a month now, because I felt I previously as Ravi mentioned, previously I had led AI teams in Google and by do. And even and having led AI teams in Big Tech, I couldn't see how it could possibly operate a team in a Big Tech company to pursue the very diverse, very different sets of opportunities that I saw and wanted to pursue. And starting different startups to pursue those valuable projects seem more efficient than having one company, even the Big Tech company, go after such a large set of resources. But having said that, I think AI and genitive AI also offers a lot of opportunities for incumbent companies, which often have a distribution advantage. Where exactly are the opportunities? So this is what I think of as the AI at the lowest level is the hardware layer. Very valuable, but also very capital intensive needs a lot of resources to build and very concentrated. So I'm sure that the valuable startups build there, but I personally don't play there because of how capital intensive and how concentrated it is. There's a call infrastructure layer, also very capital intensive, very concentrated, very valuable, but at least when I built startups, I tend not to play there. The other layer, this interesting is a developer tool in layer. So what you just saw me do was use OpenAI as a developer tool. And I see the space as hyper competitive. Look at all the startups chasing OpenAI, but there will be some mega winners. So whereas incumbents have a startup kind of a distribution advantage, I think for many startups having a technology advantage may give you a best shot at doing something meaningful there. So I personally tend to play here only when we think we have a technology advantage because that buys us a better chance to become one of the huge winners. And then with most ways of technology innovation, a lot of media attention, social media, what people tend to talk about is the tooling of the technology layer. This one of the layer that I think has got to be even more valuable and that's the application layer. Because in many ways of technology for the in-fraud and tooling layer to be successful, applications need to be built on top of them. They generate even more revenue so that they can afford to play the infrastructure layer. And what I'm seeing is that there are a lot of opportunities at the application layer where the intensity of competition is not frankly not nearly as high. Maybe just one example, I've been chanting a lot with the CEO of Mino, which is a startup that applies AI to romantic relationship coaching. And I'm an AI guy. I don't know anything about romance. And if you don't believe me, you can ask my wife, she would confirm that I don't know anything about romance. But when we decided, when we had conviction that AI could be applied to relationships, we wound up partnering with Renata Nibok, who's the former CEO of Tinder. And because she ran Tinder, she understands relationships in a very systematic way, more so than anyone else I know. So with my team providing AI expertise and her providing relationship expertise, we're able to build a pretty unique relationship mentoring application that we just announced a few weeks ago. Renata actually occasionally stops by Stanford campus and talks to Stanford students as part of her user project research. So it was possible to have seen her out. Just one last thing I'd love to go to GNA. Over the last few years, AI fund with been tuning our process for building stars. I'm just going to share that with you. So we often start off with a lot of ideas. And one example of another startup we built was Baring AI, which uses AI for smart routing of very large ocean-growing vessels. So if you're a ship captain, you just say, 20 knots or 22 knots, as I who knows. Most ship captains just make some decision. But because we're able to get global weather and ocean current data, we can make recommendations to ship captains for how to get there on time and use about 10% less fuel. But this idea was suggested to me by Mitsui, which is a major shareholder in a major shipping line that operates very large ocean-growing vessels. And this one of those things, I would never have thought of this idea myself because I've been on a boat for what do I know about global maritime shipping. But Mitsui suggested this idea to me. And we then validate the idea, make sure there's technical feasibility and marketing. Recruit the CEO. We're fortunate to find Dylan Kyle, who's a fantastic CEO with one successful exit before. And then we spend three months in our current process building a technical prototype with the CEO and doing deep customer validation. If it survives, two-thirds chance of surviving, one-third chance of not surviving, we then write a check-in that allows the company to build higher executives, build an MVP, and off it goes to raise additional rounds of capital. And I think this is what we, and so bearing AI, well now it's actually, there are not hundreds of ships on the high seas guided by bearing AI. Ships guided by bearing AI have 75 million miles, which is the equivalent of going 3,000 times around the planet and save about half a million dollars in fuel cost per ship per year in addition to six carbon emissions. I think we'll save about, I want to say about a million tons of CO2 emissions so far. But this kind of idea that what I would never come of this idea myself, but I've learned that my swim lane is AI, but when I work with experts in other sectors, they're often these exciting opportunities that are very valuable, but frankly, how many groups in the world are experts in AI and shipping or expert AI and relationships? I find that the competition intensity at the application there is often much lower. And then just one last thing, kind of just full disclosure and something that I hope all of you will do to my team's only work on projects that we think we've humanity forward. Response for AI is important, and on multiple occasions, we've killed, and I will continue to kill projects that we may assess to be financially sound, but based on ethical grounds. So lots of exciting opportunities, I think I stand for the lots of great causes you can take in entering a NELSWARE to learn about that AI tech, and then when you find applications, or go play at the infrarian tooling layer too. I think there are lots of opportunities, but I think there are what I'm seeing is, frankly, my team of AI fun with so many started ideas, we use a task management software, we use Asana to track this huge list of ideas, and it's actually quite clear to me, there are a lot more good ideas for AI businesses than people with the skill to work on them at this moment in time. So hopefully there'll be more than enough projects for everyone. There are all of you, all of us to work. Thank you. I wanted to just start off with that closing statement that you made about how there's more opportunity than there are students with skills or people with skills to pursue them, and given that we have this an audience full of students, I wanted to start off by mapping out advice for students that are entering into the university regarding AI. So if you want to pursue a career in AI right now, and let's say your child was entering Stanford, what advice would you give them in terms of how to spend their time? So, you know, there's one thing that's actually really worth doing when you're sent to students, which is take classes, it turns out that I feel like there's actually one pattern I see for both undergraduate and graduate students, including PhD students, which is there's so much exciting stuff to do, you just only jump in and do it, right? In fact, I've seen them on the grass in the freshman year, you know, try to draw on the research lab and start doing work in AI. That's okay, nothing wrong with that. But it turns out that while project work is one way to learn, coursework is I think an even more efficient way to learn, especially when it comes to mastering the fundamentals, because professors work, put a lot of work to organize the material in a way that's efficient to learn and digest. So I would say, you know, take classes in AI technology or an entrepreneurship and gain those skills. I've seen students jump in and then if you are trying to work on research lab without strong skills, you end up like labing day through or something, which is fine. You learn some things, but you actually learn a lot from taking courses. And then in addition to that, after you start to master the foundational skills, after you know how to use AI technology, or then as you start to practice, find exciting use cases across campus. I do a lot of work, you know, over with people over in climate science or in healthcare, to take my AI expertise and then marry it with a different discipline that I'm not expecting to find exciting applications. And hopefully that type of practice will help many of you find exciting projects that work on this. Do you need to take technical classes? Do you think you need to take computer science classes if you want to pursue a career in AI? Need is too strong, but I definitely encourage you to take technical classes. I think we're moving toward the world where frankly, at some future point, I think everyone should learn to code. Or rather, I think it'll be useful for everyone to learn how to code. For a couple of reasons, everyone has access to data. This is different than the world used to be even a few years ago. And especially with gendered AI, your ability to get some good work is much higher than ever before. The baritory entry is much lower than ever before. And so if you learn just a little bit of coding, the amount that you really accomplish is significantly greater than if you don't know how to code at all. And are there any skills that separate out the great AI founders? I know AI right now is like, it's a C that's rising all boats. But if you separate out the great ones from the good ones, are there any salient skills that you notice that the great AI CEOs or founders have that the good ones don't? Maybe, since you say AI, I would say is often technical death. It calls it a lot. But that one will give a different answer if you say great founders or angry AI founders. But I feel like AI is evolving rapidly. And we definitely have lost a bunch of renewals that, you know, pitch the VCs without really knowing what they're doing. And the smart VCs can sniff it out quite quickly. And it makes a huge difference. I think the technology unfortunately, you know, is like somewhat complicated for a lot of applications. So the team that actually knows what they're doing will execute an AI project 10 times faster, you know, than a team that doesn't. And 10 times is not a medium number. I literally see people take a year to do something like, oh, boy, I know that other team will have done perform this level in two weeks or maybe a month. So for many AI startups, application startups, improv startups, you kind of have to know what you're doing. So it doesn't have to be you if you're a technical co-founder. Maybe that's okay. And then second thing I see among many of the great founders is speed. I find that as a startup, you'd be surprised when you handle the great founders, the sheer speed of decision making. And you know, I sometimes talk to people from big companies and they'll say, oh, we move so fast. But when I kind of sit them side by side, how long does it take it to make decision? That's how the great founder, how long does it take me to do that? Maybe just one story. I was chatting with the Mino CEO of Renata Nibblek, former CEO Tinder. I was in the phone with her one day and she was making major architecture decisions. There's architecture and the basic two major software architectures on the consideration and the team had laid it out, let us sell some pros and cons, set a balance team with me and some of my friends and said, these are pros and cons. And then one of my my my C2 AI fund and I said, you know, we're not sure. But here are some reason we prefer architecture A. And then Renata said, okay, guys, done decision made good, and implement architecture A. And after I thought, well, Renata just make a massive engineering decision in basically 30 seconds and and and as she did. And I realized after it, I don't think there was a better way to make it because it's not as if, you know, if if the company waited another week with a higher quality decision and if it was wrong, I'm sure they would fix it, you know, the next week. But until you lift through the speed of a great business, most people, I know so many people that think their organization are fast when you stack them up to the real speed of a fast-moving CEO, they have never actually seen speed in their life. One important caveat do be responsible. I know that move fast and break things sometimes, you know, is the wrong approach. So tremendous speed when you are not being callous with people's lives and livelihood and things that could cause wrong harm. But so long as that's an important caveat of responsible AI, many of the great CEOs move faster than most people realize people can move. And so let's just double click on that on this theme of responsible AI just because I know this is a hot topic that maybe people are thinking about, which is you are clearly on the side of AI for good for responsible AI. Many of your brethren like Jeffrey Hinton and other famous leaders in the AI space have come out and are concerned that the pace of AI development will become an existential threat to humanity. So much so that famously there was a petition signed by Elon Musk and Steve Wasneyak and many thought leaders asking for the halting of the foundational, the deepest foundational models of AI for us to sort of, for society to sort of catch up. You did not sign that pledge. Can you share a little bit more detail about that? Was that a difficult decision for you to make? And can you share more details about why you didn't join them and what your philosophical is view is regarding if AI poses an existential threat? So I honestly don't see how AI poses any existential threat to the human race. We know AI can run them up. You know, self-driving cars have crashed, leading through trashy loss of life, ultimately through trading, trashy stock market. So we know, for the design software systems can have a dramatic impact and responsible AI is important. But recently I saw tells people like Jeff and others that were concerned about the question of AI extinction and I tried to understand why they thought this way. Some were worried about bad actor using AI to create a bad weapon. Others were worried about AI evolving in a way that inadvertently leads to human extinction. Similar to how we as humans have led to the extinction of many species through simple lack of awareness. Sometimes it our actions could lead to that outcome. But when I tried to assess how realistic these arguments were, I found them to be vague and non-specific about how AI could cause all. And I think that I found frustratingly frankly that trying to prove AI couldn't is akin to proving a negative and I can't prove that super intelligent AI won't be dangerous. But I can't seem to find anyone that really knows exactly how it could be. But I do know that humanity has ample experience controlling many things far more powerful than any one of us like corporations and nation states. And there are many things that we can't fully control that are nonetheless safe and valuable. Like airplanes, no one can control an airplane. It's about to live around by winds and the pilot may make a mistake. But in the early days of aviation airplanes killed many people. So we learned from those experiences, built safer aircraft, device rules by which to operate them. And today most of us can step into airplane without fearing from lies. And I think it will be like that too for AI. So I think the AI extinction, I find to be very unfortunate. What I'm seeing because doing some work in K-Tel of education as well, what I'm seeing is that kind of really unfortunately I see high school students now considering working in AI and some will say AI seems exciting, but I heard it could lead to human extinction and I just don't want to be a part of that. And so I find that the over-hite AI extinction narrative is doing real harm. Some really concerned about that. Thank you, Andrew. One more question then I'm going to open it up, which is I loved the detail on the low-hanging fruit opportunities. I know that's on everybody's all the entrepreneurs' minds of what to pursue. And so I appreciated the attention and the presentation on that. I wanted to ask about what's going to be the next big technology shift in AI because things are changing so rapidly, especially as the models now are getting smaller and open sourced. It feels like we've already conquered language. Visual AI is getting very, very good. What's next? What are you seeing that's around the corner that others might not be aware of? Yeah, you know what, done it. About several months ago I was predicting visual AI's coming next, but now everyone's all right, visualists, I guess you've got to come, something new. But in all seriousness, I think visual AI would be much more about the analysis of images rather than just generation of images. But I think where the GPT-2 moment for visual AI is not yet working, but I think it will work much better. And this will impact self-driving cars, for example, when we can finally solve problems in the long tail. And then I think actually one other thing that I wrote about just today in a newsletter called The Batch is I think one thing that many people find controversial, I think is coming as a rise of edge AI. And I know this controversial, many of us were trained to write stats off where, you know, lend yourself nice to subscription business model. How do you even find people like how do you hire engineers to write desktop applications? Like who even does that anymore? But I think that because of for various forces, including privacy, I think that in the next few years we'll see more AI applications running at the edge, meaning on your laptop, on your cell phone, something that's coming. And then I think there's just be a lot of work coming the application there as well. Okay, I want to open it up to the students. You're the reason why Andrew's here. You mentioned that on your slides you put the potential for reinforcement learning. Are the general value as a dot relative to the potential for unsupervised learning? Do you think there is still potential for generalized agents like Gato and other reinforcement learning models in society and in your AI stack for startups? So technically, last time I trained using reinforcement learning and unsupervised learning and supervised learning, but leaving that aside, I feel like I'm not convinced that reinforcement learning is near breakthrough moments, at least in the next small number of years. A lot of excitement about what we could do in reinforcement learning applied to robotics. A lot of us, you know, CS faculty, Chelsea Finn and we're running school, many others are doing exciting research there, but we do have a data problem. So it turns out that text on the internet sounds a lot like text on your documents. So we can learn from lots of text on the internet to do really well on your text documents and image on the internet, look a little bit like images that you care about. So we have a lot of data, but because every robot is different, struggling, many, many people are struggling to see how to get enough data to have the user recipe of scaling up data and compute, where for reinforcement learning, and people are working on it. Over the weekend at the CS faculty retreat, you know, there was a talk, I think, who did this talk? Shoot, thank you, it was on how to do this, or the ideas of how to do this, but I think was still a few years away from breakthroughs and those breakthroughs are reinforcement learning. But there's a great research topic, by the way, just because, you know, just because it's not working right now doesn't mean you shouldn't do research on it. So I think it's a great research topic. So I just want to know your thoughts about what are the security concerns which is coming up by you abusing that like LLM models like all these new attacks like prompt injections, data leakage, jail breaking. So what's your thought around that like, how can we like safeguard against those kind of attack because it's just starting up this new technology. So I'm assuming there's more things which will come up. Yes, so I think that for the near future, there'll be a little bit of a cat and mouse thing going on. So I think I'm seeing different companies approach this with different tools to watch out for prompt injections for data leakage. Actually, deep learning.ai is actually work of a partner on some things that hopefully will announce very soon on portfolio of tools. By the way, those of you that have not yet done it, go and go and fool around with prompt injections. See if you can get an LLM to do something. Well, don't do something actually harmful, but I actually find it kind of intellectually interesting whenever I use an LLM to prompt it to see how robust the safeguards actually are. And if actually look at the older language models a year ago, it was super easy to get the older models. Frankly, to give you detailed directions to do things that they should not give anyone detailed directions to do, but the more modern language models are much harder to install sometimes possible. Sorry, but what I'm seeing as well for a lot of corporations, a lot of corporations because of these worries will ship internal facing product first because presumably, if it says the wrong thing to your own employee, more understanding less likely with scandal, and test products internally for quite a long time, or even build capabilities for safe internal use before turning out to external use. But I do see different companies different tools for trying to adjust these. Terrific. Next question. Thank you so much. Hi there, my name is Chinat and I'm an international student from Hong Kong. I'm curious to ask because I'm hoping that after I graduate, I can hopefully go back home to work closer with family. By the same time, I feel like by going back, I'm closing a lot of doors behind me. Because for example, in Hong Kong, for example, you can't access ChatGVT without a US number, which makes access to some of these resources really difficult. So I'm curious to see what are your thoughts about navigating this complex modern landscape? Yeah, I don't want to comment on, I don't know, complicated. That's actually one thing I'm seeing. I've been to quite a few places, in Asia recently. And what I'm seeing is that many countries are developing surprisingly good capabilities for building large language model applications. The concentration of talent for genitably deep tech is very concentrated in the San Francisco Bay area. I think because there are basically two teams that did a lot of the early groundbreaking work, Google Brain, my former team, and OpenAI. And subsequently, people left and start a lot of companies here in California, being in the area. So I think that concentrated talent is very high. And it's interesting, even when I'm in Seattle, Grace City, Love to City, on weekends, you know, I hang out with friends, but the conversation is not about genitably AI. Whereas here, it kind of like, if you go to coffee shop, actually one of my friends was visiting from Taiwan. So he was hanging out with us for a week. They went back and he said, yeah, I went to coffee shop. And, you know, there was no one talking about AI. That's so weird. So at least at this moment in time, there's really heavy concentration. But I see less the deep tech layer, but the application layer, I see that skill said, developing quite quickly globally as well. Oh, and I think the opportunities in a lot of places would be local opportunities. So the shipping company that we built, we built it with Japanese company that happens to operate global lines of shipping. So I think a lot of the businesses will be playing locally whether country or that geography is strong. Those businesses will be more efficient to build in places other than Silicon Valley. Because where do I go to find a large seaport here to do that type of work? Hi, Andrew. Thank you so much for your time. My name is Komo. I wanted to ask you, if you think we'll ever reach a threshold on human dependence for AI, or if you think it'll just continue to grow exponentially. So I think we are ready, really, really depend on tech. Imagine if the internet were to shut down, I think people would die. I don't think that's exaggerating. But seriously, think about how we get food supply chain, healthcare. If the internet were to shut down, I think that will lead directly to what happens on water system, healthcare system. And I think that technology is very useful. And so long as the supplies remain reliable, I feel like it's okay to depend on technology. I mean, heck, I wish, I don't know, without dependence on agriculture system, how many of us would be able to farm and hunt enough food to keep ourselves alive? Maybe, maybe we could do it, but it's pretty challenging. So I think dependence on tech, see if there's going to keep on growing for a while. But do you think there'll be a moment where there's a difference in that relationship, not just in degree, but in kind? You know, the famous singularity point where we don't even know what we don't even know about how technology is developing? Do you think that will occur? Yeah, the technological similarity is one of those high-p things that I don't even know what it means. So it's one of those, as I think science fiction, but as an engineer and scientist, I don't know how to talk about it. It turns out there are a few terms in AI that are vague and undefined, but there are a lot of emotions, a lot of excitement about it. And I don't really know how to think about those things in a systematic rational way. But I think there's actually one thing, I think our relationship with technology is changing rapidly. Today, I probably use chat, you know, GPT-4 or Bing or BOD, pretty much every day now. And so the workflow of many people have changed. I think it keeps on changing. And do you have a view? I know this also might be more of one of these sort of hot topics that's not substantive, but on the consciousness of AI, that AI will it become conscious? Yeah, so the thing about consciousness is, is it important for the philosophical question, but I don't know of any test for whether something is conscious or not. So I think it's important philosophy and philosophy is important, but as an engineer and scientist, I don't know. There is no definition for what is conscious or not, which is, and thus we can kind of debate it at length. And there's actually one other rest formula for hype, which is if someone comes up with a very simple definition for consciousness, so someone says, oh, if you can recognize yourself in mirror, you're conscious. I make that up. It's not a good definition for consciousness. You know, wherever you're yourself, a CSM mirror, then it's actually pretty easy to get a robot to recognize yourself in a mirror. And then you can generate newspaper headlines saying AI has achieved consciousness. What it did for your kind of, you know, silly little, for your very small definition of consciousness, but that gets misinterpreted by the broader public for a grand statement than it is. So I see some of that hype in AI as well. Thank you. Next question. Hi, earlier you outlined the AI stack and recently we've seen a lot of cool things coming out of like Nvidia, Intel and other like chip companies. I'm curious on what your thoughts are on what companies like AWS and Google, like in the infrastructure layer need to do in order to make like AI and enterprises and business really effective and possible. Sure, boy. So there's a lot going on in that space. By the way, you mentioned Intel and Nvidia, I wouldn't, I think I'm actually seeing really exciting work from AMD as well. I've been pretty impressed by the MI200 MI250. I'm excited about the MI300 GPUs coming out as well. And I think the rock and stack has been coming, you know, not parity or cooler, but better than most people given credit for. But in terms of other than Google, so it turns out that if you were to use a lot of the LM startup tools, the switching calls are actually pretty low. So if you were to start with one LM API call, if you want to switch to a different LM provider, the number of lines of code is actually pretty low. So there are low switching calls. But it turns out that zero and Google Cloud and AWS are fantastic businesses because once you build on any of these clouds, you know, the switching calls tend to be very high because you have so many API host integrations. So that's why I think that a lot of the slots of selling API calls still have, you know, some work to do to find a business model that may be somewhat more defensible. I think that OpenEI's chat GPT enterprise that feels like a more defensible business than just selling API calls. But the sound was actually standard undergrad. She actually interned the curious in my lab. So a lot of Stanford roots, but I did a smart guy. I'm sure you figured out confident you figured out some good directions. Yeah, and I think even though it's in GCP and the zero or all, racing to continue to develop LM capabilities and make it easier to use and bring more customers. And yeah, it's very dynamic space. And as AI gets democratized, it feels like things are shifting more towards compute and data as predictors of success. If that's the case, do you think the locus of innovation shifts from academia to industry where the companies are going to really be dominating at the forefront of AI? Yeah, so what I'm seeing now is that there's a subset, but there's a small subset of things that are easier to do in the big tech company, which are the ones that require massive compute resources. And I do think people's perceptions are distorted because frankly, I've been on the big tech companies before, right? So I understand marketing and big tech companies, but standard big tech company marketing is, look, you need to data, you need to compute, only we have it. Why don't you just give up and don't compete with us, right? And or even, or come apply for a job and come work with us. That is, this has been the exclusive PR strategy of at least one big tech company because I know, you know, what was discussed internally exactly at that big tech company. So I would say don't buy into that marketing message. It is true that there is a subset of work that requires massive capital training and very large foundation models. That is much easier to do in the big tech company than an equity related standard. But that's a small subset of all the, all the happenings in the AI and there's plenty of work at Stanford at the application there. It turns out because of scaling laws, we're actually pretty good at predicting what happened with very large models by training on more model size models. So the very good scientific work can be done and much smaller models. And then also, you know, I routinely run kind of, you know, models on my laptop for inference, like, I don't know, when I'm on an airplane, you can run like the seven billion alarm of all the other laptop. And so there's actually a lot of stuff that you could run on your own personal computer. Thank you. Next question. Thank you, and you, so from AI expert and also the investor's perspective, so what AI driven healthcare applications do you see have the great potential to have the breakthrough in the future? And what challenges and obstacles should we be worth? Thank you. Yeah, so boy, it's a lot of complexity to that question. So I feel like, I feel like a lot of healthcare, people tend to focus on the diagnostics and the treatment. So I think lots of opportunities there. I think that the revenue model is to be sorted out. So we've seen, you know, pair an Ikeely struggle in the public markets, kind of bankruptcy, kind of levels, almost. So I think prescription digital therapeutics is definitely going through challenges. But what's the recipe for shipping AI products? And, you know, in the pay-up provide the ecosystem, what will pay us be willing to pay for? I think that many businesses are sorting that up. I think that will work. And it does actually one other huge set of options in healthcare. They think tend to be underappreciated, which is operations. Instead of the medical stuff, things like scheduling, you know, who should scheduling the MRI machine or doing kind of a patient management systems? I think that's type of healthcare operations have fewer regulatory hurdles. I think it's also a rich set of opportunities. And then lastly, does the go-to-market question of do you want to go to the market in the US or in other countries, whether right to be heard or could be very different depending on the US. Fortunately, it doesn't have as great a shortage of doctors as some of the places. And those, therefore, other places that are more amenable to your responsible, but still easier adoption of AI than the United States. Okay. I have a super-quick question. You mentioned that your team at the AI fund has so many ideas for AI applications that you have a whole son of them. What exactly is your process for generating these ideas? Oh, thanks, I'll say. And you have three seconds. Sure. We like working with subject matter experts that deeply understand the domain. It turns out that there are a lot of people in the world, including like CEOs of Fortune, Financial Companies, where really a lot of people that really understand the domain have thought deeply about something for months or even a couple years. And when we get together with them, just sometimes very happy to share their idea with us because they've been looking for someone to validate or falsify it and also to help them build it. So we actually got a lot of ideas. Some in turn up with a lot from subject matter experts that just know your Heather AI built upon there. Terrific, fantastic. Thank you Andrew so much for sharing your insights. Lots of love. Thank you for sharing your insights with Stanford's ETL Course MSN472 and the students all around the world. Everybody next week we're going to be joined by Stanford Professor Kathleen Eisenhart here at ETL physically in person. Professor Eisenhart is also the author of Simple Rules. You can find that event and other future events in this ETL series on the Stanford E Corner YouTube channel and you'll find even more of our videos, podcasts, and articles about entrepreneurship and innovation at Stanford E Corner. That's ecorner.stanpper.edu. Thank you everybody. Thank you Andrew. Thanks, thanks, Bobby. The entrepreneurial thought leader series is a Stanford E Corner original production. To learn more please visit us at ecorner.stanford.edu.