 This is the Entrepreneurial Thought Leaders Series brought to you by Stanford Equal. Hello everyone, welcome back to the Entrepreneurial Thought Leaders Series, the Stanford Seminar for aspiring entrepreneurs. ETL is presented by STVP, the Stanford Engineering Entrepreneurship Center, and bases the Business Association of Stanford Entrepreneurial Students. My name is Emily Mann, and I head up special projects in real estate and workplace services at Google. Today I'm so excited to have Daniella with me from Anthropic. Daniella, M.O.D. is the president and co-founder of Anthropic, an AI Safety and Research Company. She manages the senior leadership team, leveraging her people and management experience to further the company's goals of building reliable, interpretable, steerable AI systems. Before co-founding Anthropic in 2020, Daniella was the VP of Safety and Policy at Open AI, and managed the people and research engineering teams prior to that. Even before that, she was part of the early team at Stripe, where she managed recruiting and later led risk operations. She previously worked in international development and served as a congressional staffer. Daniella and her brother, Anthropic CEO and co-founder Dario, were named two times 100 most influential people and artificial intelligence in 2023. She received her BA in English literature, politics and music at the University of California, Santa Cruz. Everyone, please have a huge round of applause to Daniella on Valentine's Day. Thank you. All right. Well, let's start with a big question. So you are the first of this winter quarter who is a co-founder. And we've had over the course of the last 25 years in this auditorium, a number of co-founders, who visit and share their stories. And you have a particularly interesting setup with your brother. So how does that work? That's right. Well, first of all, thank you so much for having me. It's great to be here. And actually, to make it even more complicated than just being a co-founder with my brother, actually have five other co-founders. So there were seven of us total who left Open AI together. We all co-founded Anthropic. So if you're looking for a family feud, that's definitely even more interesting. Dari, when I work the most closely together of the co-founders as CEO and president, and I think I had a kind of great training wheels because I saw John and Patrick call us in. It strived work together as siblings, really kind of in the early days there. But I think what has worked especially well for us is just a ton of clarity on what are our zones of kind of ownership, competence and genius. And that's made it really easy for us to make decisions together as a unit. So what would you say are your competencies and what would you say are Dario's competencies? So if you Google CEO and president of like a public company, it's almost a perfect description of how Dario and I split our duties. So Dario is the kind of technical, actually all my co-founders basically are technical except for me, and one other. But he really is thinking about what is the kind of five-year arc of what's happening in generative AI? So much of the work that he and the team did at OpenAI was building GPT-2, GPT-3, but also a ton of sort of seminal work in technical safety research. And so much of his eye is thinking about where is the industry as a whole going? What's happening in the kind of generative AI field? What's the research strategy and direction? I'm much more sort of practical for kind of the business oriented side, but really work closely with the full senior leadership team. And think a lot about how to actually grow and scale the company of Anthropic, and then also what's happening on the product and business side. I found it's super interesting that while you're leading a technical firm and have been in technology for so many years, you actually have a philosophy in music and humanities background. So how does that work? It's so funny. I mean, I think if I could have looked forward into my future and seen what job I had, I just wouldn't have figured out how I wound up here. I think I sort of just tried to take the next best step in my career as I was kind of going along. But I do think something that's interesting is the humanities are a great way to think about not just the technology itself, which you're building, of course, requires incredible research and engineering skills, but so much of what happens in a business. And I saw this at Stripe and OpenAI and Anthropic, and certainly in politics is really about humans and how we relate to each other. And so much of how I think we scale these businesses, particularly in startups, right? Where everything is new, you're kind of building the ship or like the plane as it's taking off or to use the kind of overused metaphor. So much of that is figuring out like how to relate to managed coach and lead people. And while that's not necessarily only the province of the humanities, it's sort of much easier to kind of talk about things in the people space when you sort of come from that background. For sure, for sure. I was an engineering major here and I realized that the soft skills were so important to building product, building businesses. By the back and you could share one thing with a room full of engineering students, truly, what would you say to them to develop maybe that people's side? And their skills in leading people? That's a great question. I think, I mean, the GSB has some like touchy feeling heard about this from colleagues. Yeah, something. Anyway, I have no idea if it's good or not, but like maybe we're thinking about. I think just in general, like just figuring out like things about like how you are and how you like to work, I feel like that sounds so obvious. But like self-awareness is just one of the most useful skills in business, right? We talked a little before about the fact that I worked closely with Claire Hughes-Johnson at Nstrife and her book is amazing just to give it a shout out, scaling people. But I think something I always really sort of admired about her and I think was a useful framework and learning for me is the more you understand about how you work and what bothers you or triggers you and what you are great at and what types of people you like to work with, you'll naturally seek those people out in an organization and kind of partner with them, which is a great sort of foundational building block for figuring out how to be effective in a company or with co-founders if you start something yourself. So to get really specific, I am very, very, very impressed that you manage technical people, right? And you've managed technical people and you've managed to get them to allow you to lead and to follow you. So how do you bridge the gap when you're not necessarily in like the details of a technical project or how an architecture is executed on? And I'm asking this question because I do the same thing myself. And so, but I'm an engineer. So how do you do that? Sure. I think it, first of all, it doesn't work in every case, right? It has to sort of be the right setup, the right team. You have to have a technical person that is like a strong technical visionary because of course I can't set that vision, right? I can't tell the researchers. Hey, here's how to go train clawed to in the best ways. I just don't know that. But I do think there is a way that managing different groups of people, they're sort of our themes to it, right? Whether you're managing a giant sales organization or a group of researchers or engineers, you fundamentally are trying to get that group of people to accomplish something, right? You have to figure out what the problem is. You have to figure out how many resources does it take to fix that problem? Or what is the goal and how will you know if you're successful? And so some of the process of kind of leading technical people doesn't necessarily feel that different than leading sales people. The challenge is knowing what you don't know and sort of who to go to, right? When two kind of technical leaders or just strong technical individual contributors who were reporting to me when I was managing, you know, the language team or interpretability, you know, the challenge was sort of figuring out, okay, who's right if there's like a sort of a technical disagreement. But my default there was, you guys are going to have to talk about it and figure it out either way, right? So I probably shouldn't even be the arbiter. I should just sort of be supporting you in figuring that out. Amazing, amazing. So I want to go back as well to self awareness. Actually, I think there was a survey of GSB alums and 20 years out, 30 years out, the common sort of identification of a theme that they all said was actually self awareness, being the most important thing. So how do you yourself become more self aware? Yeah, that's great. I think there's some sort of like, these are, I would almost describe them as like boring or practical ways to sort of support that which businesses do, you know, so well. And I think startups in particular do this really well, which is we do, you know, performance reviews, which is like the least sexy sounding way to gain self awareness. I can, I can possibly say, but, but really, you know, there's something about when you work with people really closely, they actually know you really well, right? And I've had something like 12 or 15 years worth of performance reviews, which are a huge gift. But the shocking thing is like the content of those reviews, like the specifics and the details have certainly changed and sort of ebbed and flowed over the years. But the general themes are not that different, right? Like people are like very strongly who they are. And people that work closely with you will see those things about you and comment on them. So a way of cheating to get self awareness is to just ask people that are close to you, right? Brilliant. My husband probably doesn't say things that differently from like, you know, my co-founders of like, what is Daniela great at? Like what are things that she's less good at and what is she like on a good day and how in the way she is on a bad day, right? Like they'll probably all answer those questions with pretty similar themes. Yeah. Yeah. So what are you good at? What are you bad at? What would they say? I think the things, you know, the things in particular that my reports say about me is I, I simultaneously really, I'm a supportive manager. I really want people to succeed and I also hold a high bar. Those are kind of the two things I hear the most. Also ability to lead in lots of areas. It's another thing that comes up. In terms of things I'm less good at, probably unsurprisingly, I am not the expert in anything, which is actually like a feature, not about it. Yeah, but it is, it can be a drop-back, right? There's times where if something is, if there's, you know, contention between two parts of the organization, right? Product things we should, things we should do one thing, engineering things we should do something else, sales things we should do something else. I'm not necessarily deep enough in any of those disciplines to sort of say, hey, I think sales is missing this or I think that the product team sort of has this right. It's a lot of kind of general management versus specialized and there are times in the business where you need an expert. And I think really the opportunity for me is always finding the right expert. Yes, very good. Well, I know I've delayed, delayed to talk about Enthropic and I want to dive in really deeply now. So tell us more about Enthropic. Tell us about what the foundational philosophical ethic is of the company. You've innovated some really interesting things on how to make a responsive, explainable, steerable AI. Maybe we should start with an open book, like share with the audience what Enthropic is all about. Sure. So really Enthropic is aiming to build transformative general AI systems that are safe, reliable, steerable, ethical. And it's a lofty goal, right? So much of I think why we were founded was this desire to build generative AI tools and systems and products that people could use while feeling really, really confident that what we were putting on the market was trustworthy, reliable, and safe. And that's taken a number of different forms over the past few years. But I think specifically we have this sort of belief that incorporating these technical safety streams into training of the model really from day one is the best way to ensure that when they actually are in a product form and getting into the hands of customers that they're going to be safe. I love how your team has thought about using the human rights declaration as a starting point for how to really moderate content and moderate the algorithms development. Can you speak to that a little bit? Definitely. So this kind of idea that we came up with, our brilliant research team came up with, is this idea of giving Claude, which is our generative AI tool, something we call a constitution. It's called constitutional AI. And to just sort of go back a little bit to kind of how we got there, the way that you sort of used to, and by used to, I mean like three years ago, or like two years ago. A year and a half ago. You used to train these models to make sure, hey, they're not sort of saying nasty things or they're not replicating bad inherent biases, was you would do this technique called reinforcement learning from human feedback, which we also kind of co-worked on when we were at OpenAI. And reinforcement learning from human feedback is just essentially a way of like giving the model like, great, right? It's like, A plus, you said something nice, like D minus, you said something mean. And that was reasonably effective in changing some behavior of the models. But what we found was that a lot of sort of subtler things about how models respond, react, things that they're sort of deeply believing are much harder to train out in those sort of individual cases. And so we had this idea of giving the model kind of a broader constitution in the way that you would in a society to basically say, what are ways of behaving and engaging that are good for, you know, humans, right, for people, and that don't perpetuate some of the problematic things that might be in training data. And so so much of, you know, what we saw with constitutionally AI was, A, you know, we shouldn't necessarily be the arbiters of like, what is good or bad, right? We're a group of, you know, at the time, 100 people, 150 people, now 350 people, based in San Francisco, let's like think about what broader documents already exist in the world that have grappled with some of these challenges and incorporate, I think, if something like 17 different, you know, founding documents in a constitution. Amazing. You know, I find it super interesting that you've chosen, sounds like a number of global documents. And so one of the questions that come up is society is very, very diverse. And so how do we adjust for specific cultures or, you know, ethnic orientations of what good and bad is? And how do you think about that as a team because it's complicated, right? You know, how, you know, Claude might respond to, let's just take myself, an Asian American born in Canada might be very different in conversation with someone who was born, you know, in Taiwan, who has a very different, maybe view of, you know, sort of, humanity and morality. So how do you think about those challenges? Yes, this is a great question. And it's something our teams think about a lot, right, for sort of the work that we do. So I think constitutionally AI is sort of a great framework for thinking about what are the inputs that you would want to put into a model to kind of guide its ethics. But those don't necessarily have, those inputs can change, right? So depending on the culture, the country, the company even that's using our models, there's probably some degree of latitude over time that we can build in to say, hey, do you want to change sort of some of the founding documents of the model that you use? That being said, I do think there are some guardrails that we feel strongly should be in place, you know, regardless, right? Claude can't, you know, help people create weapons, right? It's like trained to ensure that you don't do something harmful to people or animals. And I think sort of regardless of where it's deployed, those are kind of, you know, company universal things that we feel, you know, believe strongly in. Another thing I'll point to is we recently did a sort of set of research that was kind of building on this constitutionally AI idea called collective constitutionally AI. And what we did there was instead of using just a set of founding documents, we actually pulled a very large kind of demographically diverse group of people from sort of around, you know, around the country and other parts of the world to see like how did they react to some of the things that were in our constitution? And what did they kind of collectively come to on some of these sort of ethical questions? And how far was the distance between sort of what our constitution said and what the collective constitution said? Wow. So more broadly than with the ethic that you have at Anthropic, how do you see your role in the broader societal efforts around governing AI in a responsible way? I mean, I was thinking about how, you know, we kind of have one chance to get it right and we can't let people down. So how do you see your role, Anthropic's role in helping to guide what AI could do for humanity? There's so much opportunity yet there is risk. Absolutely. So I think, you know, what's interesting about this is, you know, Anthropic is a company, right? We're a public-benefit corporation and we have this, you know, very lofty social mission. And I don't think anybody, including us, thinks it's right for sort of us or any particular company alone to be the arbiter of what happens with this technology. And so there's a few different ways that we try and collaborate or work with other groups to say like, how do, where, where does this decision live, right? Who is, who is kind of the group or the set of people sort of driving these outcomes? And one way that we do that is we've had a policy team basically since, since day one. The only other non-technical co-founder of ours is our policy director and he has just done an incredible job working with policymakers and, you know, government officials to really think about, you know, what are the ways that we want this technology to be, you know, regulated or what are the rules and guardrails sort of beyond the corporate level where we might want this technology to be looked at, right? Like what are the sort of insights about it that we need to share to the government and policymakers around the world to help them understand what's happening? We also work with a number of civil society and non-profit groups that work on many of the issues that you've talked about. And that's because those groups often have additional expertise that we don't necessarily have within our walls, right? There are groups that think specifically about, you know, particular elements of how this technology will be used or abused where we can lean on their expertise. Oh, very good, very good. Building on that, I, on the flip side as a user of these tools, it's not always clear. So I spent a lot of time helping non-profit organizations understand the potential of AI, but also the risks, right? And it's not always apparent that, you know, there's some challenges with these tools. It's not always accurate. Large language models can hallucinate. Depending on the data you provided, there could be bias. There could be essentially people, unfortunately, putting private information into public models. How do you see anthropics role or your role or our role of citizens in understanding how we need to show up to be good partner to these tools? Yeah. I think I have sort of a short term answer, which is, what do we do with kind of what is available to us today? And then kind of a more speculative kind of long term answer. So I think on this sort of, you know, short term front, I think there is a lot of just really interesting work being done around some of these fundamental questions, right? What does it mean to sort of use these models and understand what's happening inside them? And we try to publish our research about all of this, right? We don't have perfect information because these models are, even to the people that are trading them, still like a little bit of a mystery, right? We know, hey, you put in data, you put in compute, you do some fancine ethical algorithms, and like magic, right? You have, you have these really powerful tools. And all of the sort of details underneath are a little bit of hate still. And so I think to the degree that we are kind of able to, whether it's with, you know, customers or lawmakers or individuals sort of explaining what we've done is kind of really a big part of the ethos of Anthropic. In the longer term, I sort of particularly want to click into a research team at Anthropic in the area of mechanistic interpretability, which is an area that, again, one of my other co-founders sort of, you know, pioneered, you know, first that at Google actually, and then at OpenAI and now at Anthropic. And really the best way to think of mechanistic interpretability is almost as the equivalent of, like, neuroscience and what neuroscientists do to the human brain, what mechanistic interpretability experts do to neural networks. And so really thinking about, you know, when models, when these sort of neural networks are producing outputs, we don't know what's happening, right? Like when humans are sort of thinking, oh, like, I think this thing about this person, why do I think that, right? If we could actually go in and say, what are the literal neurons that are firing, that are causing the model to think this or do this? And are there combinations that are maybe problematic that are firing together, right? And so even if you can sort of train it out of the model at the end, it would be much better if we saw, are there kind of problematic things or positive things, right, happening in the model that we would want to adjust? Well, that's fascinating. It's almost like doing brain surgery on a neural network. Exactly. Fascinating. Fascinating. So tell us a little bit more about Cloud 2. So Cloud 2 is a wonderful tool. You should all check it out. You can try it at claw.ai. And really, you know, so much of how we have been using and thinking about sort of generative AI and sort of Cloud is, as a tool that has, that has just a wide variety of different applications. So we offer a first party API. So this is for developers to be able to build on. Also some larger customers also work with us through our first party. Cloud ai, you can play around with on your phone. You can try it on your computer. Very general assistant, but in general, the thing we've kind of heard is best at, where people most prefer using it for, is long content, you know, long writing long things, right? And up, you can upload up to 200,000 tokens of contacts. That's about two books worth of information. Cloud is great at summarizing or pulling things out. Definitely not telling you if you have an English class to cheat on your homework. That's like not what I'm saying. But don't cheat my class. Don't cheat. But Cloud is great at basically taking tone direction if you want Cloud 2 sounds like you and producing content or writing works of fiction and things like that or nonfiction. But don't use it for school, obviously. You heard all that. Just triple checking. So what are some of the other things that you've seen people use Cloud 4 that have surprised you, maybe, to? I think maybe one of the most surprising kind of macro trends that we've seen is actually very large enterprise businesses have been some of our earliest adopters. And that's generally not the kind of market adoption trajectory that most businesses see. Part of why I think that's happened is because Cloud and Anthropic kind of have this reliable sort of trustworthy scalable set of values and kind of our approach to training the models. More kind of traditional industries like insurance, healthcare, legal services, these are the types of industries that really value reliability, trustworthyness with their end customers and where things like lower hallucinations are actually a huge deal for them. Cloud is the lowest hallucination rate on our country. And so a lot of what we've seen is traditionally kind of companies that might not be the first to sort of adopt a new technology or actually some of our kind of biggest adopters. And could you maybe share an example of how a healthcare company might be using Cloud right now? Definitely. So I'll go through a few examples. So on the kind of financial services side, groups like Bridgewater are using us for financial analysis of their tools. We have a mortgage lender that is basically has a huge amount of data and is using it to help people fill out home mortgage applications and shorten the time that it takes for them to be approved. We also work with another financial services company that's using it to sort of help people figure out how to do their taxes better. In the healthcare space, what's really interesting is Cloud can be a great partner in concert with medical practitioners, a doctor or a nurse and really going through, for example, summarizing key findings from a health consult. You can't use Cloud alone today, but it often saves doctors and nurses a huge amount of just like administrative time. So much of what we hear from doctors and medical practitioners is I would love to spend more time with patients and less time doing paperwork. And Cloud is a great partner for helping them to not do as much paperwork. Yes. So on that note, what do you think about agents? The NEI agents. Yeah. Built on top of phallolums. It's a great question. I think that this is a place where there's sort of two things that are true. I think long term, I imagine this being like an incredibly powerful technology that will save many people time and labor and headache and administrative burden. I also think it's interesting that today, I don't think the models are like quite there. I agree. Which is, I think like an, it's just sort of an interesting thing to note about the sort of excitement around generative AI. I think can sometimes sort of blind us too strong of a word, but can kind of obscure the existing limitations of the technology. Right? Cloud is amazing. We use Cloud for all kinds of things at Anthropic and I use it at home and everyone uses some generative AI tool now, I think. But there are still many things that it can't do. And I think there is a way that sort of it feels when you sort of look at it and you're like, oh my god, it's like a human. It's like talking to a person. It's like, I'm not doing anything. It's not, it's not there yet. Right? There are so many things that humans cannot use generative AI to even help them with. And most applications of generative AI today, I actually think are best when done with a human in the list. Not all of them, but the majority of them. Well said. Well said. So what are your dreams for Cloud 3? It's a great question. I think for Cloud 3, number one, on some of the core kind of safety features, we have this kind of helpful, honest, harmless framework. We're hoping to just make improvements on all of those. A huge one is the number one request we hear from model quality from our customers is, how do you get the hallucination rate from X% to 0%, like whatever the number is, nobody wants a model that's going to make up information. And while we think we've seen fairly impressive gains there, it's really hard to get to 0. And so I think we're always chasing that number. Also some interesting questions around getting the models to just hedge a little bit more if they think they don't know the answer. I think on the harmlessness front, this is again a place where like, Cloud is sort of industry leading on this, but there's always more work to do and sort of understand. And then I think another set of kind of features that were sort of interested in for Cloud 3 is really, how do you make the model more generally capable, right, just sort of more intelligence, but really, how do you get it to sort of specialize in particular use cases or industries? Yeah. I really appreciate what you said about the two ends of the spectrum. On one end, you want to get the hallucination rate down to 0, which is also almost impossible. But on the other end, you want to make sure that Cloud is still useful. And I have found that with some other models, we've kind of swung to the conservative end, right? I remember in the beginning days with Gemini, it would just make stuff up, but it would give you an answer to anything, right? But now, you know, we will, it will deliberately not answer who is Emily Ma, right? Because it's ambiguous who I am truly, right? And you scour the internet. And so, where do we sit on that spectrum? Like, we want to be responsible with large language models. How do we approach that in the most honest and transparent way? So I'll start with kind of a funny story here, to sort of illustrate what you're talking about, which is in sort of early days of training, Cloud, we were really experimenting with sort of trading off some of these like, H's, right? This like, helpful, honest, harmless. You can have a perfectly harmless model if you want it. It would just not be very helpful, right? Like, you just, you know, we would sort of ask Cloud, like, who is the first president of the United States? And it would be like, I cannot answer that question. And I'm also very concerned about your well-being. And like, here is a link to like, you know, harm prevention, but say, and you're just like, Cloud, I'm fine, I promise I'm fine. So there is sort of this, there is sort of this, this like, chart of like, sort of intersection, right? Where you can say, okay, like, do we want Cloud to like risk like a little bit more helpfulness for like a little bit more honesty, right? Or a little more harmfulness, or however you might describe it. And of course, what we're always trying to do is sort of raise the watermark on all three. But at the end of the day, depending on the application, you might also want to sort of fine tune the model or train a different model for use, for certain use cases, right? You can imagine that for an educational school, for like, you know, five to eight year olds, you might want a very, very harmless version of Cloud, even if it's like a little bit less helpful. Whereas if you're using Cloud to do, for example, you know, trust and safety detection work, which is an application that many of our customers use Cloud for, so that humans don't have to look through harmful content, you actually want Cloud to be able to read and identify and understand very harmful or upsetting things and sort of filter them out. That's right. That's right. So I want to take the last couple of minutes in this sort of more formal section to talk a little bit about how anthropic came to be. So going back to the origin story. And one of the things that we grapple with as a class is what are the principles by which we live by and how do we transition well from one to another, right? And it was very clear that you and your co-founders had a vision for what could be and that the sort of circumstances that you had at OP&A, I weren't going to allow you to manifest that. How do you gracefully say no, so to speak and close out one chapter well and then start another chapter? That's a skill I feel very strongly about as entrepreneurs that we need to get good at. Whether it's when we see a product that isn't actually being adopted as much and quickly as we want and we decide to pivot or it's constantly we're iterating on our lives and having to not hold on to things for too long and be able to move forward. So how did you approach that situation that circumstance with Dario and with your other co-founders? I think what was probably the most interesting about that was when you sort of have something that you're running towards, it makes it much easier to feel grounded in your values, we felt very strongly, my six co-founders and I, we were the mostly the kind of technical safety and policy leadership of the company. And something we felt very strongly about was this kind of vision of building transformative AI systems in a way that was reliable and safe and transparent and that was like a theme that already was very like live for us because that's so much of what we worked on. And so I think in a lot of ways it just felt like a natural next progression, right? We said we have this incredible vision, we really want to be able to go and do it and I think that sort of, it's almost like in any relationship or situation when you see something that you're like this is really what I'm like meant to go next. There can be quite a lot of internal clarity that can kind of drive you forward. Yes, fantastic, fantastic. Well, I'm going to ask you one more question and I'm going to hand it to the classroom to ask many, many questions because I think they're all just bubbling ready to go. So if you were sitting here, however many years ago when you were 20 years old and you could tell yourself something, what would you tell that Denyella sitting in the front row, the 20 year old version of yourself? Oh my gosh. I think I would say number one, follow your passion and normally when people hear that they're like, oh, I like soccer, I should go be a soccer player. Whatever is the thing that is most exciting and drawing for you now, right? I didn't have a traditional career trajectory of saying I'm going to study engineering and then I'm going to go work at a tech company that was just like not how I started out. And I truly think that the time I spent working in international development and global health and I worked on a campaign and I worked in politics, I learned so much about myself and what I like to do and also what I'm good at and also very importantly what I don't like, right? I had an incredible opportunity to work on Capitol Hill and I was like, this is extremely not for me, right? It just was slow and bureaucratic and I missed the campaign days of getting to build something together with a small group of motivated people and that was such a light bulb moment for me when I essentially was like the technology industry and the startup world in particular is that, right? It's a campaign but sort of in a long term, slightly more sustainable way of doing it. So I think, I mean, it sounds so cheesy but just like, follow what is exciting for you because when you find something you love doing, you can work at it very hard for a long time and when you hate doing something like 10 hours a week of it, it's miserable let alone for you. So follow your passion. Yes. And be tuned into who you are and what authentically drives you. And know your passions might change. That's the other thing. Like when your passions change, don't hold on to old ideas of what you liked to do, be like, I thought I liked this, right? I saw myself working on Capitol Hill and I was like, oh my gosh, this is who I meant to be and then I got there and I was like, wrong turn. Fascinating. And also you weren't stuck on campaign specifically. You were able to sort of zoom out and say, these are the characteristics I loved that working on campaign. Where else could I find it? What other industries, what other areas in the world could I find that? Exactly. Similar setup. That's right. That's amazing. So I would like the TAs to get the microphones out there. Thank you so much for taking the time to talk to us. My question is on a corporate government. We saw this play out at OpenAI but I'm wondering what you guys think about what you owe to your investors while still having the strong mission of being safe? Yeah. That's a great and very timely question and I know this will shock you but we're actually asked this question quite a lot now. So Anthropic I think sort of from day one has been very curious about this question and we incorporated as a public benefit corporation exactly for this reason and public benefit corporations are very interesting for a government. I highly recommend like go Google them or S-clod. So public benefit corporations have been around for quite a while. They're a corporation, right? They're a C-Corp but they have this additional component of a public benefit mission and so like Tom, Shoes, Patagonia, these are public benefit corporations, highly profitable. And a lot of sort of what's the reason we sort of decided to go that way. We thought about a lot of different structures when we were starting Anthropic. So we were like should we be a nonprofit, should be an LLC, should be a C-Corp. And we landed on PVC because we felt that it was sort of best positioned to provide us this kind of flexibility of having investors, right? Like issuing equity, we're a normal company, we have products but there's this social mission that sort of we are somewhat legally protected from shareholder lawsuits. If we decide for example, we don't know that Claude Seven is as safe as we want it to be, we're not going to release it yet. Whereas in a traditional C-Corp, you have a much bigger risk of that being a problem if your shareholders don't like what you've done. It's not perfect but that's sort of a nice protection. Additionally, we have this group of people called the long-term benefit trust who are financially disinterested so they're not investors and they essentially elect a percentage of board seats that sit on Anthropics, traditional board of directors. And the way that we selected them was for their experience and interest and influence in the public benefit sector. And the last one I'll say is we recently published something called a Responsible Scaling Plan and this policy essentially, it's public you can read it on our website. It says, what will we do if we are concerned about sort of particular risks from either a technical safety or security side of the models that we're developing? How will we measure these and what will we do to sort of react if we're concerned that what we're going to put out into the world is going to cost harm? Thank you so much for being here. My question is how did your time at Stripe, which is a massive FinTech company led by the collision brothers, sort of prepare you to take on your leadership now yourself and how did you kind of, I guess twice in your career leaving Stripe and then also open AI and maybe other moments have the confidence to leave things that were going great behind to or maybe they weren't going great but to then do something that felt right to you. So I think on the kind of question of how did scaling at these other companies sort of help prepare for Anthropic, something that is interesting is all three of those companies are very different but hyper scale, there's some themes about that look really similar and I think in particular, helping to scale Stripe from 40 to 1500 people and then I joined opening AI around the same size, it's like this is the third time I'm watching the movie. And so there are things of course that are completely unique to Anthropic but there's like, oh, we've gone through this amount of scale, we have to add this set of processes because 300 people communicate differently than 30 people that like many of the folks I work with who have not done hyper scale before, what is happening at this, I don't know how to find information anymore, I'm totally normal. And I actually write a document every at the beginning of every year saying here's what we should expect from just a pure scaling perspective over the next year at Anthropic because I've seen it before and it's not perfect but I think there's kind of similar themes. The other thing I learned was really just I got to see such a wide view of all of those companies because I worked in so many different pockets of them that now at Anthropic I feel much more confident saying you know what I've never managed a sales team before but like I can probably figure it out and I was very grateful at both Stripe and Open AI to be given just opportunities across different parts of the company. The second part of your question I think was, repeat the second part of your question for me. Yeah, how do you know when to leave? Yeah, I love this. So it's an art and everyone will do it differently I think is my real answer. For me, I loved Stripe. Like Stripe is a great company and it was a hard decision to leave. I think again to sort of go back to this like running towards something versus away from something. The earlier part of my career had really been about sort of wanting to have like positive impacts in the world and I striped as a great company and payments is cool but it didn't sort of feel like fully meeting this other aim or goal that I had around wanting to like positively impact the world. And so when I saw the opportunity at Open AI and I knew people there I thought wow this feels like a really exciting transformative technology I kind of want to go there. And then within Thropic it was again very similar that I had this vision around you know safety and trustworthiness and felt very confident that I wanted to go do that next. Hi Danila. Thank you for the talk. So today the hot topic is AI like a couple of years ago it was NFT crypto metaverse. What's the next hot topic and are you working on it now or not? Thank you. If only I knew the future I would go found that company if I knew what it was. You know I think maybe an interesting version of your question is like is the sort of generative AI hype going to continue right like are we are we in a like over exposure period right like I see some heads nodding we're like you can only read about a technology for so long without for me like okay I get it like yeah it's changing the world. I think what's interesting is you know we've had a lot of splash and aha moments in artificial intelligence and I think there's a really interesting phase that we're entering now that I see particularly on the business side which is companies at least in 2023 were experimenting right. How can I use this technology to remove obstacles or pain in my organization or make us more effective or improve search or understand my customers better or structure data differently. And it was just very exploratory and I think what we're seeing in 2024 is just more kind of boring business scaling right we're like okay you've had access to this technology for six months or a year and people are like CFOs are now getting involved being like how much are we spending on this and like what's the actual bottom line return on it and I think what we'll see is like a little bit of a departure from just the like it can do anything language to sort of more detailed analyses of like what exactly can the technology do today and then every time there's a new model release there will be more of a discussion around what can it do now that the last model couldn't do maybe out of that come some other incredible technology that some other company will build I don't know but I think if I were to sort of make the case for what I expect to happen in AI in 2024 it would look like that. Can I do a follow up question please so I'm really curious how you guide your customers around this question of yes you could use Jenny I to do this or you could use some other very traditional method because Jenny I is not free it requires compute and on the back end I'm particularly worried about energy to run all these models so what are your thoughts on that there's a really sort of interesting conversation that I have a few times a week with a customer so number one there's so much excitement about AI right which means fortune 500 companies are like knocking down our door right they're like oh my gosh Jenny I like let us in help us and I think in many cases there is a good use case and we'll say hey it sounds like you know what you're looking for is you know help understanding complex data right and pulling out key pieces of information that would take regular search much longer and lower accuracy to do or you really want to understand analyze sentiment right that's much harder to do but there are some customers who come to us and the honest conversation we have with them is I don't think we can do anything for you right now and we don't want to sell you a bill of goods that we can to close the deal with you and get your logo right and I think there's a fundamental kind of honesty and integrity about that that feels very important yes yes and so much of what we do there is a we try to leave a good taste in their mouth to feel like we didn't lie to you right we didn't say oh AI it can solve your staffing problems like you can use it as a manager it can't do those things yet but I'm like come back to a six months from now right or will what are the areas what are the biggest problems and challenges in your business like we'll write those down and like we'll come back to you when we think that cloud can provide the solution for you but I think some just like clear off being clear-eyed about what the technology can't do and being honest with businesses about that feels important it's a great theme you know helpful harmless and honest not only applies to cloud but how you run the company and how you approach your partners employees yeah we're like are they helpful are they harmless a couple more questions oh goodness so this might be a sticky subject but I love your answer so AI is likely going to be a very decisive technology in future military conflicts and a lot of the requirements the DOD and various US government agencies have with regards to AI are a lot aligned with what you've talked about like trustworthiness transparency etc so is anthropic open to working with the US government and DOD and if so can you talk about how you would rationalize kind of this concept of doing no harm in the context of war very important question especially for a company that's working in this sort of field that we're working in the way that we do this now is we divide whether or not a business can use cloud not by the business but by the use case and so our acceptable use policy says hey if you want to use cloud for like processing you know back-end employee records or something at an institution if it's not you know sanctioned if it's not a business that we can't work with for some reason that's allowable the place that we put restrictions are on what you what the use cases are and so our current AUP says you cannot use cloud for things like military applications for weapons right there was this interesting article you probably saw of like simulation of like you know basically using these models to to do more things and cloud cloud was the least escalatory so cloud is like let's not let's not launch missiles let's like sort of downplay this let's all call it is very nice so that's obviously our hope for what we would be building into the models but the way that we think about that today is through our acceptable use policy thank you so much for being here my question is on agents as you clearly said that the models are not quite there yet do you see in the future clouds sort of becoming this all-encompassing agent that can book your flights and do everything in terms of direction of anthropic are you sort of trying to go there so I think on the agents front there's almost like a question above it that I kind of want to want to go to which is are we building kind of tools that are sort of intending to do like particular tasks or to kind of be like generalized sort of assistance are we like trying to get them to do everything and I think my real answer is like I don't think the industry knows yet and I think today it seems to me that we haven't quite like crossed the chasm between these models being great at doing sort of a set of individual tasks or maybe a combination of tasks to it understanding the sort of full scope of of like general purpose project management or something like I think that's just today sort of a bridge too far for what they can do that being said I could imagine that like Claude six or Claude seven could do that right and a component of that is sort of a product one which is how does the model kind of integrate with your you know your Google calendar and your you know flight booking and Chrome or whatever the sort of applications are but there's also just like a model capability component of it that I actually think is is the is the tougher piece of it my instinct is like we're probably some a few generations from that being possible in sort of a general way but there might be sort of narrower or simpler sets of tasks right like making a reservation somewhere might be easier than like booking a whole vacation and so I think we'll we'll probably see something like a just like a curve or like a gradient of a tool like Claude being able to do that over time but it's a great question to one more question so lucky lucky final question and you mentioned a lot of you know exciting use cases in companies and enterprises starting to adopt closed source APIs like Claude opening eye co-hear the rest of them I mean what do you think are the biggest struggles today for enterprise to continue to use emerge like create emerging workflows and use cases with these technologies and if you had to start an AI company you know facilitating this process for the enterprise specifically what would it be so I think I think there's a few I think the first is depending on the kind of amount of technical capacity within that company it can it can vary really widely so most big enterprise companies have like a technical team but some are like more technical than others and so I think there is still a little bit of a dance that happens at the sort of like integration and just like getting the sort of models started working in the guts just like a very technical sort of side of it that that is still like a process that is not nailed down I think anywhere I think like for any of the sort of LLM providers or generative AI providers on any cloud I do think that's getting better just as we sort of have more data but I actually that's like the number one most difficult part is just like plugging it into the right places to be able to like use it the way you want prompt if the way you want and sort of teach your workforce to do it the second is it's a little bit particular but the the sort of like what are the data sources that you're giving to the LLM the sort of structure of those can vary really widely and so there's also a kind of component that's almost just like figuring out what it is you're trying to plug into the model not just technically but sort of organizationally like you're like we have to pull this thing from like this system and there's this other set of data from this system that also is is almost like just another hoop and I think that it's interesting to imagine maybe there's like a bridge technology there or a bridge company there that's like we go and do all of that for you on the back end and sort of prepare you to just be plugged into one of the foundation model factories that might also come from a company like us but I think like probably that's like not the place that we're going to have the most innovation and I could definitely see an interesting business idea there. I find it's so interesting to end on this front for me with Gen AI it actually still starts with the people right it starts with the input a technology as only as good as the people and the inputs and so as we think about adopting Gen AI more broadly it's a question of what is the business need what do the people need and how do we approach it from that front so with that I know all of you are rushing off to your next class so let's give Daniel a huge round of applause thank you so fun. Next week back in this classroom we have ShiaHad who is the CEO and co-founder of our place she was also the co-founder and formerly founding CEO Malala fun so for those of you who know Malala she helped found the fund to fund education around the world and if you're interested in these events you can find it on Stanford e-corder on the YouTube channel including this one next week and lots of other videos and podcasts and whatnot so thank you again huge round of applause. The Aunt Pernurial Thought Leader's Series is a Stanford e-corder original production. To learn more please visit us at e-corder.stanford.edu